#!/usr/bin/env python3
"""è¿ç§»è¿­ä»£è¯æ®åˆ° v2 schema å¹¶è¡¥é½å¿…éœ€å­—æ®µã€‚

ç”¨æ³•:
    python scripts/iteration/migrate_iteration_evidence_to_v2.py [--dry-run]

åŠŸèƒ½:
    1. æ‰«æ docs/acceptance/evidence/*.json çš„ canonical è¯æ®æ–‡ä»¶
    2. è‹¥ $schema ä¸º v1ï¼Œåˆ™å‡çº§ä¸º v2
    3. è‹¥ç¼ºå°‘ links.regression_doc_urlï¼Œåˆ™æŒ‰è¿­ä»£å·è¡¥é½
    4. è‹¥ç¼ºå°‘ source.source_pathï¼Œåˆ™è¡¥é½ä¸ºå›å½’æ–‡æ¡£è·¯å¾„
    5. --dry-run è¾“å‡º diff ç»Ÿè®¡ï¼Œä¸å†™å…¥æ–‡ä»¶
"""

from __future__ import annotations

import argparse
import difflib
import json
import sys
from collections import OrderedDict
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterable, Optional

from scripts.iteration.iteration_evidence_naming import EVIDENCE_DIR, parse_evidence_filename
from scripts.iteration.iteration_evidence_schema import (
    CURRENT_SCHEMA_REF,
    LEGACY_SCHEMA_FILENAME,
    resolve_schema_name,
)

REPO_ROOT = Path(__file__).resolve().parent.parent.parent


@dataclass
class MigrationOutcome:
    """å•ä¸ªæ–‡ä»¶è¿ç§»ç»“æœã€‚"""

    path: Path
    changed: bool
    added_lines: int
    removed_lines: int
    warnings: list[str]


def default_regression_doc_path(iteration_number: int) -> str:
    """é»˜è®¤å›å½’æ–‡æ¡£è·¯å¾„ã€‚"""

    return f"docs/acceptance/iteration_{iteration_number}_regression.md"


def is_legacy_schema(schema_value: Optional[str]) -> bool:
    """åˆ¤æ–­ $schema æ˜¯å¦ä¸º v1ã€‚"""

    if not schema_value:
        return False
    return resolve_schema_name(schema_value) == LEGACY_SCHEMA_FILENAME


def insert_after_key(
    data: OrderedDict[str, Any],
    key: str,
    value: Any,
    after_keys: Iterable[str],
) -> OrderedDict[str, Any]:
    """åœ¨æŒ‡å®š key ä¹‹åæ’å…¥æ–° keyã€‚"""

    if key in data:
        return data
    insert_after = None
    for candidate in after_keys:
        if candidate in data:
            insert_after = candidate
            break
    if insert_after is None:
        data[key] = value
        return data
    updated: OrderedDict[str, Any] = OrderedDict()
    for item_key, item_value in data.items():
        updated[item_key] = item_value
        if item_key == insert_after:
            updated[key] = value
    return updated


def ensure_links_regression(
    data: OrderedDict[str, Any],
    regression_doc_path: str,
) -> tuple[OrderedDict[str, Any], bool, list[str]]:
    """ç¡®ä¿ links.regression_doc_url å­˜åœ¨ã€‚"""

    warnings: list[str] = []
    links = data.get("links")
    if links is None:
        links = OrderedDict()
        links["regression_doc_url"] = regression_doc_path
        data = insert_after_key(
            data,
            "links",
            links,
            ("sensitive_data_declaration", "overall_result", "commands"),
        )
        return data, True, warnings
    if not isinstance(links, dict):
        warnings.append("links å­—æ®µä¸æ˜¯å¯¹è±¡ï¼Œå·²è·³è¿‡è¡¥é½ regression_doc_url")
        return data, False, warnings
    if not isinstance(links, OrderedDict):
        links = OrderedDict(links)
        data["links"] = links
    existing = links.get("regression_doc_url")
    if not isinstance(existing, str) or not existing.strip():
        links["regression_doc_url"] = regression_doc_path
        return data, True, warnings
    return data, False, warnings


def ensure_source_path(
    data: OrderedDict[str, Any],
    source_path: str,
) -> tuple[OrderedDict[str, Any], bool, list[str]]:
    """ç¡®ä¿ source.source_path å­˜åœ¨ã€‚"""

    warnings: list[str] = []
    source = data.get("source")
    if source is None:
        source = OrderedDict()
        source["source_path"] = source_path
        data = insert_after_key(data, "source", source, ("runner",))
        return data, True, warnings
    if not isinstance(source, dict):
        warnings.append("source å­—æ®µä¸æ˜¯å¯¹è±¡ï¼Œå·²è·³è¿‡è¡¥é½ source_path")
        return data, False, warnings
    if not isinstance(source, OrderedDict):
        source = OrderedDict(source)
        data["source"] = source
    existing = source.get("source_path")
    if not isinstance(existing, str) or not existing.strip():
        source["source_path"] = source_path
        return data, True, warnings
    return data, False, warnings


def resolve_iteration_number(
    data: dict[str, Any],
    fallback: int,
) -> int:
    """è§£æ iteration_numberï¼Œå¿…è¦æ—¶å›é€€åˆ°æ–‡ä»¶åã€‚"""

    raw = data.get("iteration_number")
    if isinstance(raw, int) and raw >= 1:
        return raw
    return fallback


def resolve_regression_doc_path(data: dict[str, Any], fallback: str) -> str:
    """ä¼˜å…ˆä½¿ç”¨å·²æœ‰ links.regression_doc_urlã€‚"""

    links = data.get("links")
    if isinstance(links, dict):
        value = links.get("regression_doc_url")
        if isinstance(value, str) and value.strip():
            return value
    return fallback


def render_json(data: OrderedDict[str, Any]) -> str:
    """æ¸²æŸ“ JSON æ–‡æœ¬ã€‚"""

    return json.dumps(data, ensure_ascii=False, indent=2) + "\n"


def diff_stats(old_text: str, new_text: str) -> tuple[int, int]:
    """è®¡ç®— diff çš„æ–°å¢/åˆ é™¤è¡Œæ•°ã€‚"""

    diff = difflib.unified_diff(
        old_text.splitlines(),
        new_text.splitlines(),
        lineterm="",
    )
    added = 0
    removed = 0
    for line in diff:
        if line.startswith("+++ ") or line.startswith("--- ") or line.startswith("@@"):
            continue
        if line.startswith("+"):
            added += 1
        elif line.startswith("-"):
            removed += 1
    return added, removed


def format_path(path: Path) -> str:
    """æ ¼å¼åŒ–è¾“å‡ºè·¯å¾„ï¼ˆå°½é‡ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰ã€‚"""

    try:
        return str(path.relative_to(REPO_ROOT))
    except ValueError:
        return str(path)


def migrate_record(
    data: OrderedDict[str, Any],
    iteration_number: int,
) -> tuple[OrderedDict[str, Any], bool, list[str]]:
    """å¯¹å•ä¸ªè¯æ®è®°å½•æ‰§è¡Œè¿ç§»ã€‚"""

    changed = False
    warnings: list[str] = []

    schema_value = data.get("$schema")
    if isinstance(schema_value, str) and is_legacy_schema(schema_value):
        data["$schema"] = CURRENT_SCHEMA_REF
        changed = True

    default_path = default_regression_doc_path(iteration_number)
    regression_path = resolve_regression_doc_path(data, default_path)

    data, links_changed, links_warnings = ensure_links_regression(data, default_path)
    warnings.extend(links_warnings)
    if links_changed:
        changed = True

    data, source_changed, source_warnings = ensure_source_path(data, regression_path)
    warnings.extend(source_warnings)
    if source_changed:
        changed = True

    return data, changed, warnings


def migrate_file(path: Path, iteration_number: int, *, dry_run: bool) -> MigrationOutcome:
    """è¿ç§»å•ä¸ªè¯æ®æ–‡ä»¶ã€‚"""

    raw_text = path.read_text(encoding="utf-8")
    data = json.loads(raw_text, object_pairs_hook=OrderedDict)
    if not isinstance(data, OrderedDict):
        raise ValueError("JSON æ ¹å¯¹è±¡å¿…é¡»ä¸º object")

    normalized_iteration = resolve_iteration_number(data, iteration_number)
    migrated, changed, warnings = migrate_record(data, normalized_iteration)
    if not changed:
        return MigrationOutcome(path, False, 0, 0, warnings)

    new_text = render_json(migrated)
    added, removed = diff_stats(raw_text, new_text)

    if not dry_run:
        path.write_text(new_text, encoding="utf-8")

    return MigrationOutcome(path, True, added, removed, warnings)


def main() -> int:
    """CLI å…¥å£ã€‚"""

    parser = argparse.ArgumentParser(
        description="è¿ç§» iteration evidence åˆ° v2 schema å¹¶è¡¥é½å­—æ®µ",
    )
    parser.add_argument(
        "--evidence-dir",
        type=Path,
        default=EVIDENCE_DIR,
        help="è¯æ®ç›®å½•ï¼ˆé»˜è®¤: docs/acceptance/evidenceï¼‰",
    )
    parser.add_argument(
        "--dry-run",
        "-n",
        action="store_true",
        help="é¢„è§ˆæ¨¡å¼ï¼Œè¾“å‡º diff ç»Ÿè®¡ï¼Œä¸å†™å…¥æ–‡ä»¶",
    )
    args = parser.parse_args()

    evidence_dir: Path = args.evidence_dir
    if not evidence_dir.exists():
        print(f"âŒ è¯æ®ç›®å½•ä¸å­˜åœ¨: {evidence_dir}", file=sys.stderr)
        return 1

    json_paths = sorted(evidence_dir.glob("*.json"))
    outcomes: list[MigrationOutcome] = []
    errors: list[str] = []

    for path in json_paths:
        try:
            parsed = parse_evidence_filename(path.name)
        except ValueError:
            continue
        if not parsed.get("is_canonical"):
            continue
        iteration_number = int(parsed["iteration_number"])
        try:
            outcome = migrate_file(path, iteration_number, dry_run=args.dry_run)
            outcomes.append(outcome)
        except (OSError, ValueError, json.JSONDecodeError) as exc:
            errors.append(f"{path}: {exc}")

    if errors:
        print("âŒ è¿ç§»è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯:", file=sys.stderr)
        for err in errors:
            print(f"  - {err}", file=sys.stderr)
        return 1

    changed_files = [o for o in outcomes if o.changed]
    added_total = sum(o.added_lines for o in changed_files)
    removed_total = sum(o.removed_lines for o in changed_files)

    if args.dry_run:
        if not changed_files:
            print("ğŸ” [DRY-RUN] æœªå‘ç°éœ€è¦è¿ç§»çš„æ–‡ä»¶")
            return 0
        for outcome in changed_files:
            rel_path = format_path(outcome.path)
            print(f"ğŸ” [DRY-RUN] {rel_path}: +{outcome.added_lines} -{outcome.removed_lines}")
        print()
        print(
            f"ğŸ” [DRY-RUN] å˜æ›´æ–‡ä»¶æ•°: {len(changed_files)}ï¼Œæ–°å¢ {added_total} è¡Œï¼Œåˆ é™¤ {removed_total} è¡Œ"
        )
        return 0

    if not changed_files:
        print("âœ… æœªå‘ç°éœ€è¦è¿ç§»çš„æ–‡ä»¶")
        return 0

    for outcome in changed_files:
        rel_path = format_path(outcome.path)
        print(f"âœ… å·²æ›´æ–°: {rel_path}")
        for warning in outcome.warnings:
            print(f"âš ï¸  {rel_path}: {warning}")

    print()
    print(f"å®Œæˆè¿ç§»: {len(changed_files)} ä¸ªæ–‡ä»¶")
    return 0


if __name__ == "__main__":
    sys.exit(main())
