# -*- coding: utf-8 -*-
"""
scm_sync_status - SCM åŒæ­¥çŠ¶æ€æ‘˜è¦æ ¸å¿ƒå®ç°

åŠŸèƒ½:
- ä»æ•°æ®åº“èšåˆåŒæ­¥çŠ¶æ€æ‘˜è¦
- è®¡ç®— error_budget
- èšåˆç†”æ–­å™¨çŠ¶æ€
- è¾“å‡º Prometheus æŒ‡æ ‡æ ¼å¼

è®¾è®¡åŸåˆ™:
- çº¯ä¸šåŠ¡é€»è¾‘ï¼Œä¸åŒ…å« argparse/æ‰“å°
- CLI å…¥å£åœ¨æ ¹ç›®å½• scm_sync_status.py

ä½¿ç”¨ç¤ºä¾‹:
    from engram.logbook.scm_sync_status import (
        get_sync_summary,
        format_prometheus_metrics,
    )

    with db.get_connection() as conn:
        summary = get_sync_summary(conn)
        metrics = format_prometheus_metrics(summary)
"""

from __future__ import annotations

import json
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Dict, List
from urllib.parse import urlparse

from psycopg.rows import dict_row


class InvariantSeverity(str, Enum):
    """ä¸å˜é‡è¿è§„ä¸¥é‡ç¨‹åº¦"""

    CRITICAL = "critical"  # éœ€è¦ç«‹å³å¤„ç†
    WARNING = "warning"  # éœ€è¦å…³æ³¨
    INFO = "info"  # ä¿¡æ¯æ€§æç¤º


@dataclass
class InvariantViolation:
    """
    ä¸å˜é‡è¿è§„è®°å½•

    è¡¨ç¤ºæ£€æµ‹åˆ°çš„ä¸€ä¸ªå¥åº·æ£€æŸ¥è¿è§„é¡¹ã€‚
    """

    check_id: str  # æ£€æŸ¥é¡¹ ID
    name: str  # æ£€æŸ¥é¡¹åç§°
    severity: InvariantSeverity  # ä¸¥é‡ç¨‹åº¦
    count: int  # è¿è§„æ•°é‡
    description: str  # æè¿°
    remediation_hint: str  # ä¿®å¤å»ºè®®
    details: List[Dict[str, Any]] = field(default_factory=list)  # è¯¦ç»†è®°å½•

    def to_dict(self) -> Dict[str, Any]:
        return {
            "check_id": self.check_id,
            "name": self.name,
            "severity": self.severity.value,
            "count": self.count,
            "description": self.description,
            "remediation_hint": self.remediation_hint,
            "details": self.details,
        }


@dataclass
class HealthCheckResult:
    """
    å¥åº·æ£€æŸ¥ç»“æœ

    åŒ…å«æ‰€æœ‰æ£€æŸ¥é¡¹çš„ç»“æœå’Œæ•´ä½“å¥åº·çŠ¶æ€ã€‚
    """

    healthy: bool  # æ˜¯å¦å¥åº·ï¼ˆæ—  critical è¿è§„ï¼‰
    violations: List[InvariantViolation] = field(default_factory=list)
    checked_at: float = field(default_factory=time.time)
    total_checks: int = 0
    passed_checks: int = 0
    failed_checks: int = 0

    @property
    def exit_code(self) -> int:
        """
        è¿”å› CLI é€€å‡ºç 

        - 0: å¥åº·ï¼ˆæ— è¿è§„ï¼‰
        - 1: æœ‰ warning çº§åˆ«è¿è§„
        - 2: æœ‰ critical çº§åˆ«è¿è§„
        """
        if not self.violations:
            return 0

        has_critical = any(v.severity == InvariantSeverity.CRITICAL for v in self.violations)
        if has_critical:
            return 2

        has_warning = any(v.severity == InvariantSeverity.WARNING for v in self.violations)
        if has_warning:
            return 1

        return 0

    def to_dict(self) -> Dict[str, Any]:
        return {
            "healthy": self.healthy,
            "exit_code": self.exit_code,
            "checked_at": self.checked_at,
            "total_checks": self.total_checks,
            "passed_checks": self.passed_checks,
            "failed_checks": self.failed_checks,
            "violations": [v.to_dict() for v in self.violations],
        }


__all__ = [
    "get_sync_summary",
    "format_prometheus_metrics",
    "check_invariants",
    "format_health_check_output",
    "HealthCheckResult",
    "InvariantViolation",
    "InvariantSeverity",
    # å†…éƒ¨å‡½æ•°ï¼ˆä¾›æµ‹è¯•ä½¿ç”¨ï¼‰
    "_load_error_budget",
    "_load_circuit_breakers",
    "_parse_circuit_breaker_key",
    "_aggregate_circuit_breakers",
    "_load_rate_limit_buckets",
    "_legacy_token_buckets",
    "_load_pauses",
    "_default_error_budget",
]


# ============ error_budget èšåˆï¼ˆä» scm.sync_runs è¯»å–ï¼‰ ============


def _load_error_budget(conn, *, window_minutes: int = 60, db_api=None) -> Dict[str, Any]:
    """
    ä» scm.sync_runs è¡¨èšåˆ error_budget ç»Ÿè®¡

    è¯»å– counts / error_summary_json / request_stats å­—æ®µï¼Œè®¡ç®—:
    - failure count/rate
    - rate_limit_429 count/rate
    - timeout count/rate

    Args:
        conn: æ•°æ®åº“è¿æ¥
        window_minutes: ç»Ÿè®¡çª—å£ï¼ˆåˆ†é’Ÿï¼‰
        db_api: æ•°æ®åº“ API æ¨¡å—ï¼ˆç”¨äºæµ‹è¯•æ³¨å…¥ï¼‰

    Returns:
        error_budget å­—å…¸ï¼Œç»“æ„ä¸ _default_error_budget() ä¸€è‡´
    """
    if db_api is None:
        from engram.logbook import scm_db as db_api

    # ä½¿ç”¨ db.get_sync_runs_health_stats è·å–å¥åº·ç»Ÿè®¡
    health_stats = db_api.get_sync_runs_health_stats(
        conn,
        window_minutes=window_minutes,
        window_count=100,  # æœ€å¤šç»Ÿè®¡æœ€è¿‘ 100 æ¬¡è¿è¡Œ
    )

    total_runs = health_stats.get("total_runs", 0)
    failed_count = health_stats.get("failed_count", 0)
    completed_count = health_stats.get("completed_count", 0)
    total_requests = health_stats.get("total_requests", 0)
    total_429_hits = health_stats.get("total_429_hits", 0)
    total_timeout_count = health_stats.get("total_timeout_count", 0)

    # è®¡ç®—å„é¡¹æ¯”ç‡
    failure_rate = failed_count / total_runs if total_runs > 0 else 0.0
    rate_limit_rate = total_429_hits / total_requests if total_requests > 0 else 0.0
    timeout_rate = total_timeout_count / total_requests if total_requests > 0 else 0.0

    return {
        "window_minutes": window_minutes,
        "samples": total_runs,
        "total_requests": total_requests,
        "completed_runs": completed_count,
        "failure": {
            "count": failed_count,
            "rate": failure_rate,
        },
        "rate_limit_429": {
            "count": total_429_hits,
            "rate": rate_limit_rate,
        },
        "timeout": {
            "count": total_timeout_count,
            "rate": timeout_rate,
        },
    }


def _load_circuit_breakers(conn) -> List[Dict[str, Any]]:
    """åŠ è½½ç†”æ–­å™¨çŠ¶æ€"""
    rows: List[Dict[str, Any]] = []
    with conn.cursor(row_factory=dict_row) as cur:
        cur.execute(
            "SELECT key, value_json FROM logbook.kv WHERE namespace = %s",
            ("scm.sync_health",),
        )
        for row in cur.fetchall():
            state = row["value_json"] or {}
            if isinstance(state, str):
                state = json.loads(state)
            rows.append(
                {
                    "key": row["key"],
                    "state": state,
                }
            )
    return rows


def _parse_circuit_breaker_key(key: str) -> Dict[str, str]:
    """
    è§£æç†”æ–­å™¨ keyï¼Œæå– project_key å’Œ scope

    key æ ¼å¼: <project_key>:<scope>
    scope å¯ä»¥æ˜¯:
    - 'global'
    - 'instance:<instance_key>'
    - 'tenant:<tenant_id>'
    - 'pool:<pool_name>'

    Args:
        key: ç†”æ–­å™¨ key å­—ç¬¦ä¸²

    Returns:
        åŒ…å« project_key, scope, scope_type, scope_value çš„å­—å…¸
    """
    parts = key.split(":", 1)
    project_key = parts[0] if parts else "default"
    scope = parts[1] if len(parts) > 1 else "global"

    # è¿›ä¸€æ­¥è§£æ scope ç±»å‹
    scope_type = "global"
    scope_value = ""

    if scope == "global":
        scope_type = "global"
    elif scope.startswith("instance:"):
        scope_type = "instance"
        scope_value = scope[9:]  # å»æ‰ 'instance:' å‰ç¼€
    elif scope.startswith("tenant:"):
        scope_type = "tenant"
        scope_value = scope[7:]  # å»æ‰ 'tenant:' å‰ç¼€
    elif scope.startswith("pool:"):
        scope_type = "pool"
        scope_value = scope[5:]  # å»æ‰ 'pool:' å‰ç¼€
    else:
        # å‘åå…¼å®¹ï¼šæ—§æ ¼å¼å¯èƒ½æ²¡æœ‰å‰ç¼€
        scope_type = "unknown"
        scope_value = scope

    return {
        "project_key": project_key,
        "scope": scope,
        "scope_type": scope_type,
        "scope_value": scope_value,
    }


def _aggregate_circuit_breakers(entries: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    æŒ‰ scope èšåˆç†”æ–­å™¨çŠ¶æ€

    ä½¿ç”¨ build_circuit_breaker_key è§„èŒƒè§£æ keyï¼Œæå– scope_type è¿›è¡Œåˆ†ç»„ç»Ÿè®¡ã€‚
    """
    by_scope: Dict[str, Any] = {}
    total_open = 0
    total_half_open = 0

    for entry in entries:
        key = entry["key"]
        parsed = _parse_circuit_breaker_key(key)
        parsed["scope"]
        scope_type = parsed["scope_type"]

        state = entry.get("state") or {}
        state_value = state.get("state", "closed")

        if state_value == "open":
            total_open += 1
        elif state_value == "half_open":
            total_half_open += 1

        # æŒ‰ scope_type åˆ†ç»„ï¼ˆglobal/instance/tenant/poolï¼‰
        scope_data = by_scope.setdefault(
            scope_type,
            {
                "count": 0,
                "open_count": 0,
                "half_open_count": 0,
                "closed_count": 0,
                "total_failures": 0,
                "entries": [],
                # å‘åå…¼å®¹æ—§å­—æ®µå
                "open": 0,
                "failure_count": 0,
                "success_count": 0,
            },
        )
        scope_data["count"] += 1

        if state_value == "open":
            scope_data["open_count"] += 1
            scope_data["open"] += 1  # å‘åå…¼å®¹
        elif state_value == "half_open":
            scope_data["half_open_count"] += 1
        else:
            scope_data["closed_count"] += 1

        failure_count = int(state.get("failure_count", 0) or 0)
        success_count = int(state.get("success_count", 0) or 0)
        scope_data["total_failures"] += failure_count
        scope_data["failure_count"] += failure_count  # å‘åå…¼å®¹
        scope_data["success_count"] += success_count  # å‘åå…¼å®¹

        # æ·»åŠ è§£æåçš„ scope ä¿¡æ¯åˆ° entry
        enriched_entry = {
            **entry,
            "parsed_key": parsed,
        }
        scope_data["entries"].append(enriched_entry)

    return {
        "by_scope": by_scope,
        "total_count": len(entries),
        "total_open": total_open,
        "total_half_open": total_half_open,
    }


def _load_rate_limit_buckets(conn) -> List[Dict[str, Any]]:
    """åŠ è½½é€Ÿç‡é™åˆ¶æ¡¶çŠ¶æ€"""
    buckets: List[Dict[str, Any]] = []
    with conn.cursor(row_factory=dict_row) as cur:
        cur.execute(
            """
            SELECT instance_key, tokens, rate, burst, paused_until, meta_json
            FROM scm.sync_rate_limits
            """
        )
        now = datetime.now(timezone.utc)
        for row in cur.fetchall():
            meta = row["meta_json"] or {}
            if isinstance(meta, str):
                meta = json.loads(meta)
            pause_until = row["paused_until"]
            is_paused = False
            pause_remaining = 0.0
            if pause_until:
                if pause_until.tzinfo is None:
                    pause_until = pause_until.replace(tzinfo=timezone.utc)
                is_paused = pause_until > now
                pause_remaining = max(0.0, (pause_until - now).total_seconds())
            tokens = float(row["tokens"])
            rate = float(row["rate"])
            wait_seconds = max(0.0, (1.0 - tokens) / rate) if rate > 0 and tokens < 1.0 else 0.0
            buckets.append(
                {
                    "instance_key": row["instance_key"],
                    "tokens_remaining": tokens,
                    "pause_until": pause_until.isoformat() if pause_until else None,
                    "source": meta.get("pause_source") or meta.get("source"),
                    "is_paused": is_paused,
                    "pause_remaining_seconds": pause_remaining,
                    "wait_seconds": wait_seconds,
                    "rate": rate,
                    "burst": int(row["burst"]),
                }
            )
    return buckets


def _legacy_token_buckets(buckets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """è½¬æ¢ä¸ºæ—§ç‰ˆ token bucket æ ¼å¼"""
    states: List[Dict[str, Any]] = []
    for b in buckets:
        states.append(
            {
                "instance_key": b["instance_key"],
                "tokens_remaining": b["tokens_remaining"],
                "paused_until": b["pause_until"],
                "wait_seconds": b["wait_seconds"],
                "is_paused": b["is_paused"],
                "pause_remaining_seconds": b["pause_remaining_seconds"],
                "rate": b["rate"],
                "burst": b["burst"],
            }
        )
    return states


def _load_pauses(conn) -> Dict[str, Any]:
    """åŠ è½½æš‚åœçŠ¶æ€"""
    paused_details: List[Dict[str, Any]] = []
    pauses_by_reason: Dict[str, int] = {}
    now = time.time()
    with conn.cursor(row_factory=dict_row) as cur:
        cur.execute(
            "SELECT key, value_json FROM logbook.kv WHERE namespace = %s",
            ("scm.sync_pauses",),
        )
        for row in cur.fetchall():
            value = row["value_json"] or {}
            if isinstance(value, str):
                value = json.loads(value)
            reason_code = value.get("reason_code") or "unknown"
            pauses_by_reason[reason_code] = pauses_by_reason.get(reason_code, 0) + 1
            paused_until = float(value.get("paused_until", 0.0))
            remaining = max(0.0, paused_until - now)
            paused_details.append(
                {
                    "repo_id": value.get("repo_id"),
                    "job_type": value.get("job_type"),
                    "reason": value.get("reason"),
                    "reason_code": reason_code,
                    "paused_until": paused_until,
                    "paused_at": value.get("paused_at"),
                    "remaining_seconds": remaining,
                    "is_expired": remaining <= 0,
                }
            )
    paused_repos = {
        d["repo_id"] for d in paused_details if d["repo_id"] is not None and not d["is_expired"]
    }
    return {
        "pauses_by_reason": pauses_by_reason,
        "paused_details": paused_details,
        "paused_repos_count": len(paused_repos),
    }


def _default_error_budget() -> Dict[str, Any]:
    """è¿”å›é»˜è®¤çš„ error_budget ç»“æ„"""
    return {
        "window_minutes": 60,
        "samples": 0,
        "total_requests": 0,
        "completed_runs": 0,
        "failure": {"count": 0, "rate": 0.0},
        "rate_limit_429": {"count": 0, "rate": 0.0},
        "timeout": {"count": 0, "rate": 0.0},
    }


def get_sync_summary(conn, *, db_api=None) -> Dict[str, Any]:
    """
    è·å–åŒæ­¥çŠ¶æ€æ‘˜è¦

    èšåˆä»¥ä¸‹ä¿¡æ¯ï¼š
    - ä»“åº“ç»Ÿè®¡
    - è¿è¡ŒçŠ¶æ€ç»Ÿè®¡
    - ä»»åŠ¡çŠ¶æ€ç»Ÿè®¡
    - é”çŠ¶æ€
    - ç†”æ–­å™¨çŠ¶æ€
    - é€Ÿç‡é™åˆ¶çŠ¶æ€
    - é”™è¯¯é¢„ç®—
    - æš‚åœçŠ¶æ€

    Args:
        conn: æ•°æ®åº“è¿æ¥
        db_api: æ•°æ®åº“ API æ¨¡å—ï¼ˆç”¨äºæµ‹è¯•æ³¨å…¥ï¼‰

    Returns:
        æ‘˜è¦å­—å…¸
    """
    if db_api is None:
        from engram.logbook import scm_db as db_api

    base = db_api.get_sync_status_summary(conn)
    jobs = db_api.list_sync_jobs(conn, limit=100)
    repos = {r["repo_id"]: r for r in db_api.list_repos(conn, limit=500)}
    for job in jobs:
        repo = repos.get(job.get("repo_id"))
        instance_key = ""
        tenant_id = ""
        if repo:
            url = repo.get("url") or ""
            parsed = urlparse(url)
            instance_key = parsed.netloc or url
            tenant_id = repo.get("project_key") or ""
        job["instance_key"] = instance_key
        job["tenant_id"] = tenant_id
    locks = db_api.list_sync_locks(conn, limit=100)
    expired_locks = db_api.list_expired_locks(conn, limit=100)
    circuit_states = _load_circuit_breakers(conn)
    circuit_breakers = _aggregate_circuit_breakers(circuit_states)
    rate_limit_buckets = _load_rate_limit_buckets(conn)
    token_bucket_states = _legacy_token_buckets(rate_limit_buckets)
    pauses_info = _load_pauses(conn)

    summary = {
        "repos_count": base["repos_count"],
        "repos_by_type": base["repos_by_type"],
        "runs_24h_by_status": base["runs_24h_by_status"],
        "jobs_by_status": base["jobs_by_status"],
        "locks": locks,
        "expired_locks": expired_locks,
        "cursors_count": base["cursors_count"],
        "jobs": jobs,
        "window_stats": {},
        "by_instance": {},
        "by_tenant": {},
        "top_lag_repos": [],
        "circuit_breakers": circuit_breakers,
        "rate_limit_buckets": rate_limit_buckets,
        "error_budget": _load_error_budget(conn, db_api=db_api),
        "pauses_by_reason": pauses_info["pauses_by_reason"],
        # å…¼å®¹æ—§å­—æ®µ
        "circuit_breaker_states": circuit_states,
        "token_bucket_states": token_bucket_states,
        "paused_by_reason": pauses_info["pauses_by_reason"],
        "paused_repos_count": pauses_info["paused_repos_count"],
        "paused_details": pauses_info["paused_details"],
    }
    return summary


def format_prometheus_metrics(summary: Dict[str, Any]) -> str:
    """
    å°†æ‘˜è¦æ ¼å¼åŒ–ä¸º Prometheus æŒ‡æ ‡æ ¼å¼

    Args:
        summary: åŒæ­¥çŠ¶æ€æ‘˜è¦

    Returns:
        Prometheus æŒ‡æ ‡æ–‡æœ¬
    """
    lines: List[str] = []

    # base metrics
    lines.append(f"scm_repos_total {summary.get('repos_count', 0)}")
    for repo_type, count in summary.get("repos_by_type", {}).items():
        lines.append(f'scm_repos_by_type{{repo_type="{repo_type}"}} {count}')
    jobs_total = sum(summary.get("jobs_by_status", {}).values())
    lines.append(f"scm_jobs_total {jobs_total}")
    lines.append(f"scm_expired_locks {len(summary.get('expired_locks', []))}")
    lines.append(f"scm_cursors_total {summary.get('cursors_count', 0)}")

    # error_budget metrics
    eb = summary.get("error_budget", {})
    lines.append(f"scm_error_budget_samples {eb.get('samples', 0)}")
    lines.append(f"scm_error_budget_failure_count {eb.get('failure', {}).get('count', 0)}")
    lines.append(f"scm_error_budget_failure_rate {eb.get('failure', {}).get('rate', 0)}")
    lines.append(f"scm_error_budget_429_count {eb.get('rate_limit_429', {}).get('count', 0)}")
    lines.append(f"scm_error_budget_429_rate {eb.get('rate_limit_429', {}).get('rate', 0)}")
    lines.append(f"scm_error_budget_timeout_count {eb.get('timeout', {}).get('count', 0)}")
    lines.append(f"scm_error_budget_timeout_rate {eb.get('timeout', {}).get('rate', 0)}")

    # circuit breaker metrics
    for cb in summary.get("circuit_breaker_states", []):
        key = cb.get("key", "")
        state = cb.get("state", {})
        lines.append(f'scm_breaker_state{{instance_key="{key}"}} 1')
        lines.append(f'scm_breaker_failure_count{{key="{key}"}} {state.get("failure_count", 0)}')
        lines.append(f'scm_breaker_success_count{{key="{key}"}} {state.get("success_count", 0)}')
        lines.append(f'scm_circuit_breaker_state{{key="{key}"}} 1')
        lines.append(
            f'scm_circuit_breaker_failure_count{{key="{key}"}} {state.get("failure_count", 0)}'
        )
        lines.append(
            f'scm_circuit_breaker_success_count{{key="{key}"}} {state.get("success_count", 0)}'
        )

    circuit_breakers = summary.get("circuit_breakers", {})
    lines.append(f"scm_circuit_breakers_by_scope {len(circuit_breakers.get('by_scope', {}))}")
    lines.append(
        f"scm_circuit_breakers_total_failures {sum(v.get('failure_count', 0) for v in circuit_breakers.get('by_scope', {}).values())}"
    )

    # rate limit buckets (new)
    for bucket in summary.get("rate_limit_buckets", []):
        key = bucket["instance_key"]
        lines.append(
            f'scm_rate_limit_bucket_tokens{{instance_key="{key}"}} {bucket["tokens_remaining"]}'
        )
        lines.append(
            f'scm_rate_limit_bucket_paused{{instance_key="{key}"}} {1 if bucket["is_paused"] else 0}'
        )
        lines.append(
            f'scm_rate_limit_bucket_pause_seconds{{instance_key="{key}"}} {bucket["pause_remaining_seconds"]}'
        )

    # token bucket metrics (legacy)
    for bucket in summary.get("token_bucket_states", []):
        key = bucket["instance_key"]
        lines.append(
            f'scm_token_bucket_tokens_remaining{{instance_key="{key}"}} {bucket["tokens_remaining"]}'
        )
        lines.append(
            f'scm_token_bucket_wait_seconds{{instance_key="{key}"}} {bucket["wait_seconds"]}'
        )
        lines.append(
            f'scm_token_bucket_is_paused{{instance_key="{key}"}} {1 if bucket["is_paused"] else 0}'
        )
        lines.append(
            f'scm_token_bucket_pause_remaining_seconds{{instance_key="{key}"}} {bucket["pause_remaining_seconds"]}'
        )

    # rate_limit pause_until metric
    for bucket in summary.get("token_bucket_states", []):
        if bucket["paused_until"]:
            try:
                ts = datetime.fromisoformat(bucket["paused_until"]).timestamp()
            except Exception:
                ts = 0
        else:
            ts = 0
        lines.append(
            f'scm_rate_limit_pause_until{{instance_key="{bucket["instance_key"]}"}} {int(ts)}'
        )

    # pauses by reason
    for reason, count in summary.get("pauses_by_reason", {}).items():
        lines.append(f'scm_pauses_by_reason{{reason_code="{reason}"}} {count}')
        lines.append(f'scm_paused_by_reason{{reason_code="{reason}"}} {count}')
    lines.append(f"scm_paused_repos_total {summary.get('paused_repos_count', 0)}")

    # jobs by status
    for status, count in summary.get("jobs_by_status", {}).items():
        lines.append(f'scm_jobs_by_status{{status="{status}"}} {count}')

    # retry backoff seconds (from jobs with not_before)
    now = datetime.now(timezone.utc)
    for job in summary.get("jobs", []):
        not_before = job.get("not_before")
        if not not_before:
            continue
        if not_before.tzinfo is None:
            not_before = not_before.replace(tzinfo=timezone.utc)
        if not_before <= now:
            continue
        backoff_seconds = (not_before - now).total_seconds()
        instance_key = job.get("instance_key", "")
        tenant_id = job.get("tenant_id", "")
        job_type = job.get("job_type", "")
        lines.append(
            f'scm_retry_backoff_seconds{{instance_key="{instance_key}",tenant_id="{tenant_id}",job_type="{job_type}"}} {backoff_seconds}'
        )

    return "\n".join(lines) + "\n"


# ============ å¥åº·æ£€æŸ¥ä¸å˜é‡ ============


def check_invariants(
    conn,
    *,
    db_api=None,
    include_details: bool = False,
    grace_seconds: int = 60,
) -> HealthCheckResult:
    """
    æ‰§è¡Œç³»ç»Ÿå¥åº·ä¸å˜é‡æ£€æŸ¥

    æ£€æŸ¥é¡¹ï¼š
    1. expired_running_jobs: running jobs ä¸­ locked_at+lease å·²è¿‡æœŸæ•°é‡
    2. orphan_locks: sync_locks è¿‡æœŸ/å­¤ç«‹ï¼ˆé”å­˜åœ¨ä½†æ— å¯¹åº” running jobï¼‰æ•°é‡
    3. gitlab_jobs_missing_dimensions: active gitlab_* jobs ç¼ºå¤± gitlab_instance/tenant_id
    4. expired_pauses: paused_records è¿‡æœŸä½†ä»å­˜åœ¨äºæ•°æ®åº“
    5. circuit_breaker_inconsistencies: circuit_breaker state ä¸ error_budget çš„çŸ›ç›¾çŠ¶æ€

    Args:
        conn: æ•°æ®åº“è¿æ¥
        db_api: æ•°æ®åº“ API æ¨¡å—ï¼ˆç”¨äºæµ‹è¯•æ³¨å…¥ï¼‰
        include_details: æ˜¯å¦åŒ…å«è¯¦ç»†è®°å½•ï¼ˆé»˜è®¤ False ä»¥å‡å°‘è¾“å‡ºï¼‰
        grace_seconds: running job è¿‡æœŸå®½é™æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        HealthCheckResult: å¥åº·æ£€æŸ¥ç»“æœ
    """
    if db_api is None:
        from engram.logbook import scm_db as db_api

    violations: List[InvariantViolation] = []
    total_checks = 5
    passed_checks = 0

    # æ£€æŸ¥ 1: expired_running_jobs
    expired_running_count = db_api.count_expired_running_jobs(conn, grace_seconds=grace_seconds)
    if expired_running_count > 0:
        details = []
        if include_details:
            expired_jobs = db_api.list_expired_running_jobs(
                conn, grace_seconds=grace_seconds, limit=10
            )
            details = [
                {"job_id": str(j["job_id"]), "repo_id": j["repo_id"], "job_type": j["job_type"]}
                for j in expired_jobs
            ]
        violations.append(
            InvariantViolation(
                check_id="expired_running_jobs",
                name="è¿‡æœŸçš„ Running ä»»åŠ¡",
                severity=InvariantSeverity.CRITICAL,
                count=expired_running_count,
                description=f"æœ‰ {expired_running_count} ä¸ª running çŠ¶æ€çš„ä»»åŠ¡ç§Ÿçº¦å·²è¿‡æœŸ",
                remediation_hint="è¿è¡Œ `engram-scm-sync reaper --once` å›æ”¶è¿‡æœŸä»»åŠ¡",
                details=details,
            )
        )
    else:
        passed_checks += 1

    # æ£€æŸ¥ 2: orphan_locks
    orphan_lock_count = db_api.count_orphan_locks(conn)
    if orphan_lock_count > 0:
        details = []
        if include_details:
            orphan_locks = db_api.list_orphan_locks(conn, limit=10)
            details = [
                {
                    "lock_id": lock["lock_id"],
                    "repo_id": lock["repo_id"],
                    "job_type": lock["job_type"],
                }
                for lock in orphan_locks
            ]
        violations.append(
            InvariantViolation(
                check_id="orphan_locks",
                name="å­¤ç«‹é”",
                severity=InvariantSeverity.WARNING,
                count=orphan_lock_count,
                description=f"æœ‰ {orphan_lock_count} ä¸ªé”æ²¡æœ‰å¯¹åº”çš„ running job",
                remediation_hint="è¿è¡Œ `engram-scm-sync admin locks force-release --lock-id <id>` é‡Šæ”¾å­¤ç«‹é”",
                details=details,
            )
        )
    else:
        passed_checks += 1

    # æ£€æŸ¥ 3: gitlab_jobs_missing_dimensions
    missing_dims_count = db_api.count_gitlab_jobs_missing_dimensions(conn)
    if missing_dims_count > 0:
        details = []
        if include_details:
            missing_jobs = db_api.list_gitlab_jobs_missing_dimensions(conn, limit=10)
            details = [
                {
                    "job_id": str(j["job_id"]),
                    "repo_id": j["repo_id"],
                    "job_type": j["job_type"],
                    "gitlab_instance": j.get("gitlab_instance"),
                    "tenant_id": j.get("tenant_id"),
                }
                for j in missing_jobs
            ]
        violations.append(
            InvariantViolation(
                check_id="gitlab_jobs_missing_dimensions",
                name="GitLab ä»»åŠ¡ç¼ºå¤±ç»´åº¦",
                severity=InvariantSeverity.WARNING,
                count=missing_dims_count,
                description=f"æœ‰ {missing_dims_count} ä¸ª gitlab_* ä»»åŠ¡ç¼ºå¤± gitlab_instance æˆ– tenant_id åˆ—",
                remediation_hint="æ£€æŸ¥ scheduler å…¥é˜Ÿé€»è¾‘ï¼Œç¡®ä¿ payload ä¸­åŒ…å«ç»´åº¦ä¿¡æ¯ï¼›å¯ä½¿ç”¨ SQL è¡¥å¡«ç»´åº¦åˆ—",
                details=details,
            )
        )
    else:
        passed_checks += 1

    # æ£€æŸ¥ 4: expired_pauses
    expired_pause_count = db_api.count_expired_pauses_affecting_scheduling(conn)
    if expired_pause_count > 0:
        details = []
        if include_details:
            expired_pauses = db_api.list_expired_pauses(conn, limit=10)
            details = expired_pauses
        violations.append(
            InvariantViolation(
                check_id="expired_pauses",
                name="è¿‡æœŸçš„æš‚åœè®°å½•",
                severity=InvariantSeverity.INFO,
                count=expired_pause_count,
                description=f"æœ‰ {expired_pause_count} ä¸ªå·²è¿‡æœŸçš„æš‚åœè®°å½•ä»åœ¨æ•°æ®åº“ä¸­",
                remediation_hint="è¿è¡Œæ¸…ç†è„šæœ¬åˆ é™¤è¿‡æœŸçš„ scm.sync_pauses è®°å½•ï¼Œæˆ–ç­‰å¾…è‡ªåŠ¨æ¸…ç†",
                details=details,
            )
        )
    else:
        passed_checks += 1

    # æ£€æŸ¥ 5: circuit_breaker_inconsistencies
    cb_inconsistencies = db_api.get_circuit_breaker_inconsistencies(conn)
    if cb_inconsistencies:
        violations.append(
            InvariantViolation(
                check_id="circuit_breaker_inconsistencies",
                name="ç†”æ–­å™¨çŠ¶æ€ä¸ä¸€è‡´",
                severity=InvariantSeverity.WARNING,
                count=len(cb_inconsistencies),
                description=f"æœ‰ {len(cb_inconsistencies)} ä¸ªç†”æ–­å™¨çŠ¶æ€ä¸ error_budget ä¸ä¸€è‡´",
                remediation_hint="è¿è¡Œ `engram-scm-sync admin jobs reset-dead` é‡ç½®æ­»ä»»åŠ¡ï¼Œæˆ–æ‰‹åŠ¨æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€",
                details=cb_inconsistencies if include_details else [],
            )
        )
    else:
        passed_checks += 1

    # ç¡®å®šæ•´ä½“å¥åº·çŠ¶æ€
    has_critical = any(v.severity == InvariantSeverity.CRITICAL for v in violations)
    healthy = not has_critical

    return HealthCheckResult(
        healthy=healthy,
        violations=violations,
        checked_at=time.time(),
        total_checks=total_checks,
        passed_checks=passed_checks,
        failed_checks=total_checks - passed_checks,
    )


def format_health_check_output(result: HealthCheckResult, *, verbose: bool = False) -> str:
    """
    æ ¼å¼åŒ–å¥åº·æ£€æŸ¥ç»“æœä¸ºäººç±»å¯è¯»æ–‡æœ¬

    Args:
        result: å¥åº·æ£€æŸ¥ç»“æœ
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

    Returns:
        str: æ ¼å¼åŒ–çš„æ–‡æœ¬è¾“å‡º
    """
    lines: List[str] = []

    # æ ‡é¢˜å’ŒçŠ¶æ€
    status_icon = "âœ“" if result.healthy else "âœ—"
    status_text = "å¥åº·" if result.healthy else "ä¸å¥åº·"
    lines.append(f"å¥åº·æ£€æŸ¥ç»“æœ: {status_icon} {status_text}")
    lines.append(
        f"æ£€æŸ¥æ—¶é—´: {datetime.fromtimestamp(result.checked_at, tz=timezone.utc).isoformat()}"
    )
    lines.append(f"æ£€æŸ¥é¡¹: {result.passed_checks}/{result.total_checks} é€šè¿‡")
    lines.append("")

    if not result.violations:
        lines.append("æ‰€æœ‰æ£€æŸ¥é¡¹å‡é€šè¿‡ã€‚")
    else:
        lines.append("è¿è§„é¡¹:")
        for v in result.violations:
            severity_icon = {
                InvariantSeverity.CRITICAL: "ğŸ”´",
                InvariantSeverity.WARNING: "ğŸŸ¡",
                InvariantSeverity.INFO: "ğŸ”µ",
            }.get(v.severity, "âšª")

            lines.append(f"  {severity_icon} [{v.severity.value.upper()}] {v.name}")
            lines.append(f"     æ•°é‡: {v.count}")
            lines.append(f"     æè¿°: {v.description}")
            lines.append(f"     å»ºè®®: {v.remediation_hint}")

            if verbose and v.details:
                lines.append("     è¯¦æƒ…:")
                for i, d in enumerate(v.details[:5]):
                    lines.append(f"       {i + 1}. {d}")
                if len(v.details) > 5:
                    lines.append(f"       ... è¿˜æœ‰ {len(v.details) - 5} æ¡")
            lines.append("")

    return "\n".join(lines)
