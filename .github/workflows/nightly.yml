# ==============================================================================
# Engram Nightly Build - Full Tier æµ‹è¯•
# ==============================================================================
#
# å®šæ—¶æ‰§è¡Œ Full Tier æµ‹è¯•ï¼ˆæ¯æ—¥ UTC 02:00ï¼Œå³åŒ—äº¬æ—¶é—´ 10:00ï¼‰
# ä¹Ÿå¯é€šè¿‡ workflow_dispatch æ‰‹åŠ¨è§¦å‘
#
# ==============================================================================
# Full å±‚åˆ†å±‚ç­–ç•¥è¯´æ˜Ž (2026-01 æ›´æ–°)
# ==============================================================================
# Full å±‚ (Nightly å¿…è·‘):
#   - verify-unified VERIFY_FULL=1ï¼ˆå«é™çº§æµ‹è¯•ï¼‰
#   - step3-migrate-dry-runï¼ˆè¿ç§»è„šæœ¬éªŒè¯ï¼‰
#   - dual-read testï¼ˆprimary/shadow åŽç«¯ä¸€è‡´æ€§ï¼‰
#   - MinIO integrationï¼ˆå¯é…ç½®å¼€å…³: run_minio_testsï¼‰
#   - Docker build verificationï¼ˆå¯é…ç½®å¼€å…³: run_buildï¼‰
#
# çŽ¯å¢ƒå˜é‡:
#   - RUN_INTEGRATION_TESTS=1
#   - ä¸å¯ç”¨ HTTP_ONLY_MODEï¼ˆå…è®¸é™çº§æµ‹è¯•ï¼‰
#   - VERIFY_FULL=1
#   - SKIP_JSONRPC ä¿æŒæœªè®¾ç½®/false
#
# Allowed Failure ç­–ç•¥:
#   - openmemory_upstream_drift_check: ä¿æŒ warning/éžé˜»å¡ž
#   - Security priority (exit=1): åœ¨ Summary ä¸­çªå‡ºæ˜¾ç¤ºï¼Œè§¦å‘é€šçŸ¥
#
# ==============================================================================
#
# ==============================================================================
# Nightly å±‚ Step3 æµ‹è¯•å†…å®¹ (åˆå§‹æ ¡å‡†é¢„ç®— 2026-01)
# ==============================================================================
# Step3 PGVector Integration:
#   - make test-step3-pgvector                 â‰¤10min
#     å®Œæ•´è¿è¡Œ PGVector é›†æˆæµ‹è¯•ï¼ˆéœ€è¦ PostgreSQLï¼‰
#     åŒ…å«: test_pgvector_backend_integration + test_pgvector_e2e_minimal
#
# Step3 Collection Migrate (dry-run):
#   - make step3-migrate-dry-run               â‰¤5min (ä¸¤ç§ç­–ç•¥åˆè®¡)
#     è¿è¡Œ: shared-table + table-per-collection ä¸¤ç§ç­–ç•¥ dry-run
#     éªŒè¯è¿ç§»è„šæœ¬é€»è¾‘æ­£ç¡®æ€§ï¼Œä¸å®žé™…ä¿®æ”¹æ•°æ®åº“
#     è¾“å‡º: .artifacts/step3-migrate/*.json (å« duration_seconds)
#
# Step3 Smoke Test (Full Mode):
#   - make step3-run-smoke                     â‰¤10min
#     çŽ¯å¢ƒå˜é‡: STEP3_SKIP_CHECK=0ï¼ˆæ‰§è¡Œ seek_consistency_checkï¼‰
#     Nightly å±‚æ‰§è¡Œå®Œæ•´ä¸€è‡´æ€§æ£€æŸ¥ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
#     åŒ…å«: ç´¢å¼•åŒæ­¥(â‰¤3min) + æ£€ç´¢éªŒè¯(â‰¤1min) + ä¸€è‡´æ€§æ£€æŸ¥(â‰¤4min) + çŽ¯å¢ƒ(â‰¤2min)
#
# Step3 Dual-Read Test:
#   - python -m seek_query --dual-read          â‰¤10min
#     éªŒè¯ primary (single_table) å’Œ shadow (per_table) åŽç«¯ä¸€è‡´æ€§
#     æ‰§è¡Œå›ºå®šæŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯”è¾ƒ overlap_ratio å’Œ score_diff
#     è¾“å‡º: .artifacts/dual-read/*.json
#
# Step3 PGVector Migration Drill Test:
#   - make test-step3-pgvector-migration-drill  â‰¤15min
#     å®Œæ•´çš„ per_table -> shared_table è¿ç§»æ¼”ç»ƒ
#     æµ‹è¯•å†…å®¹: æ•°æ®å†™å…¥ã€è¿ç§»æ‰§è¡Œã€éš”ç¦»æ€§éªŒè¯ã€dry-runã€å†²çªç­–ç•¥
#     è¾“å‡º: .artifacts/test-results/step3-pgvector-migration-drill.xml
#     å¤±è´¥æ—¶: .artifacts/step3-diagnostics/migration-drill-diagnostics.txt
#
# Step3 Nightly Rebuild (æ ‡å‡†åŒ–æµç¨‹):
#   - make step3-nightly-rebuild                 â‰¤30min
#     æ ‡å‡†åŒ–ç´¢å¼•é‡å»ºæµç¨‹ï¼ˆNightly å¼ºæ‰§è¡Œï¼‰:
#     1. ä¿å­˜å½“å‰ active collectionï¼ˆç”¨äºŽå›žæ»šï¼‰
#     2. full rebuild ç”Ÿæˆå¸¦ version_tag çš„æ–° collectionï¼ˆä¸è¦†ç›–æ—§ï¼‰
#     3. æ‰§è¡Œ seek_query --query-set nightly_default é—¨ç¦æ£€æŸ¥
#     4. é—¨ç¦é€šè¿‡åŽè°ƒç”¨ set_active_collection æ¿€æ´»
#     5. å¤±è´¥è¾“å‡ºæ˜Žç¡®å›žæ»šæŒ‡ä»¤ï¼ˆå›žåˆ°æ—§ active_collectionï¼‰
#     è¾“å‡º: .artifacts/step3-nightly-rebuild/nightly-rebuild.json
#
# é¢„ç®—æ›´æ–°è¯´æ˜Ž:
#   - å¦‚å®žé™…è¿è¡Œè¶…æ—¶ï¼Œéœ€æ£€æŸ¥æ˜¯å¦æœ‰æ€§èƒ½å›žå½’æˆ–æµ‹è¯•ç”¨ä¾‹å¢žåŠ 
#   - é¢„ç®—åŸºäºŽ GitHub Actions ubuntu-latest runner é¢„ä¼°
#   - è€—æ—¶æ•°æ®å¯ä»Ž artifact junit xml å’Œ migrate JSON ä¸­æå–
# ==============================================================================

name: Nightly Build

on:
  schedule:
    # æ¯æ—¥ UTC 02:00 (åŒ—äº¬æ—¶é—´ 10:00)
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      run_build:
        description: 'è¿è¡Œ Docker æž„å»ºéªŒè¯ï¼ˆverify-buildï¼‰'
        required: false
        default: true
        type: boolean
      run_degradation:
        description: 'è¿è¡Œé™çº§æµ‹è¯•ï¼ˆVERIFY_FULL=1ï¼‰'
        required: false
        default: true
        type: boolean
      run_minio_tests:
        description: 'è¿è¡Œ MinIO é›†æˆæµ‹è¯•ï¼ˆtest-step1-integrationï¼‰'
        required: false
        default: true
        type: boolean
      run_step3_pgvector_tests:
        description: 'è¿è¡Œ Step3 PGVector é›†æˆæµ‹è¯•'
        required: false
        default: true
        type: boolean
      run_step3_migration_rehearsal:
        description: 'è¿è¡Œ Step3 è¿ç§»æ¼”ç»ƒï¼ˆdry-runï¼‰'
        required: false
        default: true
        type: boolean
      run_step3_smoke_with_check:
        description: 'è¿è¡Œ Step3 Smoke æµ‹è¯•ï¼ˆå«ä¸€è‡´æ€§æ£€æŸ¥ï¼‰'
        required: false
        default: true
        type: boolean
      run_dual_read_test:
        description: 'è¿è¡Œ Step3 åŒè¯»æ¯”è¾ƒæµ‹è¯•ï¼ˆdual-readï¼‰'
        required: false
        default: true
        type: boolean
      run_step3_migration_drill:
        description: 'è¿è¡Œ Step3 PGVector è¿ç§»æ¼”ç»ƒé›†æˆæµ‹è¯•'
        required: false
        default: true
        type: boolean
      run_nightly_rebuild:
        description: 'è¿è¡Œ Step3 Nightly Rebuildï¼ˆfull rebuild + gate + activateï¼‰'
        required: false
        default: true
        type: boolean
      openmemory_freeze_override:
        description: 'OpenMemory å‡çº§å†»ç»“ Overrideï¼ˆéœ€é…åˆ override_reasonï¼‰'
        required: false
        default: false
        type: boolean
      openmemory_freeze_override_reason:
        description: 'Override åŽŸå› ï¼ˆå¦‚ï¼šSecurity update for CVE-XXXXï¼‰'
        required: false
        default: ''
        type: string

# å…¨å±€çŽ¯å¢ƒå˜é‡
env:
  COMPOSE_PROJECT_NAME: engram
  COMPOSE_FILE: docker-compose.unified.yml
  # æœåŠ¡è´¦å·å¯†ç 
  STEP1_MIGRATOR_PASSWORD: ci_migrator_pass_123
  STEP1_SVC_PASSWORD: ci_svc_pass_123
  OPENMEMORY_MIGRATOR_PASSWORD: ci_om_migrator_pass_123
  OPENMEMORY_SVC_PASSWORD: ci_om_svc_pass_123
  # PostgreSQL é…ç½®
  POSTGRES_USER: postgres
  POSTGRES_PASSWORD: postgres
  POSTGRES_DB: engram
  # MinIO é…ç½®
  MINIO_ROOT_USER: minioadmin
  MINIO_ROOT_PASSWORD: minioadmin

jobs:
  # ============================================================================
  # ä¸»æµç¨‹ï¼šDeploy â†’ Integration Tests â†’ Verification (Full å±‚)
  # ============================================================================
  nightly-full:
    name: Nightly Full Test Suite
    runs-on: ubuntu-latest
    timeout-minutes: 60
    # Full å±‚ä»…åœ¨ schedule æˆ– workflow_dispatch è§¦å‘æ—¶æ‰§è¡Œ
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    env:
      # --------------------------------------------------------------------------
      # Full å±‚çŽ¯å¢ƒå˜é‡
      # - å¯ç”¨é›†æˆæµ‹è¯•ï¼Œä¸å¯ç”¨ HTTP_ONLYï¼ˆå…è®¸ JSON-RPC + é™çº§æµ‹è¯•ï¼‰
      # --------------------------------------------------------------------------
      RUN_INTEGRATION_TESTS: "1"
      VERIFY_FULL: "1"
      # HTTP_ONLY_MODE ä¸è®¾ç½®ï¼ˆé»˜è®¤ falseï¼Œå…è®¸ JSON-RPCï¼‰
      # SKIP_DEGRADATION_TEST ä¸è®¾ç½®ï¼ˆé»˜è®¤ falseï¼Œæ‰§è¡Œé™çº§æµ‹è¯•ï¼‰
      # SKIP_JSONRPC ä¿æŒæœªè®¾ç½® (default: false)

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-nightly-${{ hashFiles('apps/**/requirements*.txt', 'apps/**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-nightly-
            ${{ runner.os }}-pip-

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest psycopg[binary] httpx minio
          # Step1 dependencies
          cd apps/step1_logbook_postgres/scripts
          pip install -r requirements.txt
          pip install -e .
          # Gateway dependencies
          cd ../../../apps/step2_openmemory_gateway/gateway
          pip install -e '.[dev]'

      # ========================================================================
      # Step 1: Deploy unified stack
      # ========================================================================
      - name: Deploy unified stack
        id: deploy
        run: |
          echo "::group::Deploy unified stack"
          make deploy
          echo "::endgroup::"
          
          echo "Waiting for services to stabilize..."
          sleep 15
          
          echo "::group::Service status"
          docker compose -p engram -f docker-compose.unified.yml ps
          echo "::endgroup::"

      # ========================================================================
      # Step 2: MinIO Integration Tests (Optional via input)
      # ========================================================================
      - name: Run Step1 Integration Tests (MinIO)
        id: step1_integration
        if: ${{ github.event_name == 'schedule' || github.event.inputs.run_minio_tests == 'true' }}
        timeout-minutes: 20
        run: |
          echo "::group::Step1 Integration Tests with MinIO"
          # åœæ­¢çŽ°æœ‰æœåŠ¡ï¼Œä½¿ç”¨ MinIO profile é‡å¯
          docker compose -p engram -f docker-compose.unified.yml down
          
          # ä½¿ç”¨ MinIO + test profile è¿è¡Œé›†æˆæµ‹è¯•
          make test-step1-integration 2>&1 | tee /tmp/step1-integration-output.txt
          echo "::endgroup::"

      - name: Collect Step1 Integration logs on failure
        if: failure() && steps.step1_integration.outcome == 'failure'
        run: |
          mkdir -p .artifacts/step1-integration
          cp /tmp/step1-integration-output.txt .artifacts/step1-integration/ 2>/dev/null || true
          docker compose -p engram -f docker-compose.unified.yml --profile minio --profile test config > .artifacts/step1-integration/compose-config.yml 2>&1 || true
          docker compose -p engram -f docker-compose.unified.yml --profile minio --profile test ps > .artifacts/step1-integration/compose-ps.txt 2>&1 || true
          docker compose -p engram -f docker-compose.unified.yml --profile minio --profile test logs --no-color > .artifacts/step1-integration/compose-logs.txt 2>&1 || true
          # Collect pytest reports if exist
          cp -r apps/step1_logbook_postgres/scripts/.pytest_cache .artifacts/step1-integration/ 2>/dev/null || true
          cp apps/step1_logbook_postgres/scripts/pytest-report.* .artifacts/step1-integration/ 2>/dev/null || true

      # ========================================================================
      # Step 3: Re-deploy for Unified Stack Verification
      # ========================================================================
      - name: Re-deploy unified stack for verification
        id: redeploy
        if: success() || steps.step1_integration.outcome == 'skipped'
        run: |
          echo "::group::Re-deploy unified stack"
          # æ¸…ç† MinIO profile çš„æœåŠ¡
          docker compose -p engram -f docker-compose.unified.yml --profile minio --profile test down 2>/dev/null || true
          
          # é‡æ–°éƒ¨ç½²æ ‡å‡†æ ˆ
          make deploy
          echo "::endgroup::"
          
          echo "Waiting for services to stabilize..."
          sleep 15
          
          echo "::group::Service status"
          docker compose -p engram -f docker-compose.unified.yml ps
          echo "::endgroup::"

      # ========================================================================
      # Step 4: Unified Stack Verification (with degradation tests)
      # ========================================================================
      - name: Verify unified stack
        id: verify_unified
        timeout-minutes: 15
        run: |
          echo "::group::Unified Stack Verification"
          mkdir -p .artifacts/unified-stack
          VERIFY_FULL_FLAG=""
          if [ "${{ github.event_name }}" == "schedule" ] || [ "${{ github.event.inputs.run_degradation }}" == "true" ]; then
            VERIFY_FULL_FLAG="VERIFY_FULL=1"
            echo "Running with FULL mode (including degradation tests)"
          fi
          
          make verify-unified ${VERIFY_FULL_FLAG} VERIFY_JSON_OUT=.artifacts/unified-stack/verify-results.json 2>&1 | tee /tmp/verify-unified-output.txt
          echo "::endgroup::"

      # ========================================================================
      # OpenMemory Patch Sync Check
      # ========================================================================
      - name: Run OpenMemory patch sync check
        id: openmemory_patch_check
        timeout-minutes: 5
        env:
          # ===================================================================
          # è·¯å¾„ B ç­–ç•¥ (2026-01)ï¼šNightly ä¸å¼ºåˆ¶è¦æ±‚ patch æ–‡ä»¶
          # ===================================================================
          # - é»˜è®¤: OPENMEMORY_PATCH_FILES_REQUIRED=0ï¼ˆéžä¸¥æ ¼æ¨¡å¼ï¼‰
          # - ä¸¥æ ¼æ ¡éªŒä»…åœ¨ä»¥ä¸‹åœºæ™¯å¯ç”¨ï¼ˆé€šè¿‡ workflow_dispatch è¾“å…¥ï¼‰:
          #   1. æ˜Žç¡®è¦æ±‚ patch æ–‡ä»¶æ ¡éªŒ (ä¾‹å¦‚ release åˆ†æ”¯å‡†å¤‡)
          #   2. upstream_ref å˜æ›´åŽéœ€è¦éªŒè¯ patch ä¸€è‡´æ€§
          # - å¦‚éœ€å¼ºåˆ¶æ ¡éªŒï¼Œå¯æ‰‹åŠ¨è§¦å‘ workflow å¹¶è®¾ç½®ç›¸åº”è¾“å…¥å‚æ•°
          # ===================================================================
          OPENMEMORY_PATCH_FILES_REQUIRED: "0"
        run: |
          echo "::group::OpenMemory Patch Sync Check"
          mkdir -p .artifacts/openmemory-patch-conflicts
          
          # è¿è¡Œä¸€è‡´æ€§æ£€æŸ¥ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼Œpatch æ–‡ä»¶ç¼ºå¤±/æ ¡éªŒå¤±è´¥ä¼šå¯¼è‡´é”™è¯¯ï¼‰
          python scripts/openmemory_sync.py check --json > .artifacts/openmemory-sync-check-report.json 2>&1 || true
          
          # ä½¿ç”¨ manual ç­–ç•¥ç”Ÿæˆå†²çªæ–‡ä»¶ä»¥ä¾¿åˆ†æž
          python scripts/openmemory_sync.py apply --dry-run --strategy manual --json > .artifacts/openmemory-sync-apply-report.json 2>&1 || true
          
          # è¿è¡Œè¡¥ä¸è½åœ°æ ¡éªŒ
          python scripts/openmemory_sync.py verify --json > .artifacts/openmemory-sync-verify-report.json 2>&1 || true
          
          # æ˜¾ç¤ºæ±‡æ€»
          echo "=== Check Report ===" 
          cat .artifacts/openmemory-sync-check-report.json | head -100 || true
          echo ""
          echo "=== Apply Report ===" 
          cat .artifacts/openmemory-sync-apply-report.json | head -100 || true
          echo ""
          echo "=== Verify Report ==="
          cat .artifacts/openmemory-sync-verify-report.json | head -100 || true
          echo "::endgroup::"

      - name: Upload OpenMemory patch conflicts (always, before potential fail)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: openmemory-patch-conflicts-${{ github.run_number }}
          path: |
            .artifacts/openmemory-patch-conflicts/
            .artifacts/openmemory-sync-check-report.json
            .artifacts/openmemory-sync-apply-report.json
            .artifacts/openmemory-sync-verify-report.json
          retention-days: 14
          if-no-files-found: ignore

      - name: Parse OpenMemory patch sync results
        id: patch_sync_parse
        if: always()
        run: |
          echo "=== Parse OpenMemory Patch Sync Results ==="
          python scripts/openmemory_sync_parse.py \
            --check .artifacts/openmemory-sync-check-report.json \
            --apply .artifacts/openmemory-sync-apply-report.json \
            --verify .artifacts/openmemory-sync-verify-report.json \
            --github-output $GITHUB_OUTPUT

      - name: Fail if OpenMemory patch sync has errors (after artifacts upload)
        if: always()
        run: |
          # ä»Žè§£æžè„šæœ¬è¾“å‡ºä¸­è¯»å–çŠ¶æ€
          SHOULD_FAIL="${{ steps.patch_sync_parse.outputs.should_fail }}"
          SHOULD_WARN="${{ steps.patch_sync_parse.outputs.should_warn }}"
          OVERALL_STATUS="${{ steps.patch_sync_parse.outputs.overall_status }}"
          SUMMARY="${{ steps.patch_sync_parse.outputs.summary }}"
          
          # è¾“å‡ºå…³é”®å­—æ®µ
          echo "=== OpenMemory Patch Sync Status ==="
          echo "Overall Status: $OVERALL_STATUS"
          echo "Should Fail: $SHOULD_FAIL"
          echo "Should Warn: $SHOULD_WARN"
          echo "Summary: $SUMMARY"
          echo ""
          echo "Verify çŠ¶æ€:"
          echo "  - verify_final_status: ${{ steps.patch_sync_parse.outputs.verify_final_status }}"
          echo "  - category_mismatch_A: ${{ steps.patch_sync_parse.outputs.category_mismatch_A }}"
          echo "  - category_mismatch_B: ${{ steps.patch_sync_parse.outputs.category_mismatch_B }}"
          echo "  - category_mismatch_C: ${{ steps.patch_sync_parse.outputs.category_mismatch_C }}"
          echo "  - missing_count: ${{ steps.patch_sync_parse.outputs.missing_count }}"
          echo "  - conflict_files_count: ${{ steps.patch_sync_parse.outputs.conflict_files_count }}"
          echo "  - conflict_artifacts_dir: ${{ steps.patch_sync_parse.outputs.conflict_artifacts_dir }}"
          echo "  - strict_patch_files: ${{ steps.patch_sync_parse.outputs.strict_patch_files }}"
          echo ""
          echo "Apply çŠ¶æ€:"
          echo "  - apply_final_status: ${{ steps.patch_sync_parse.outputs.apply_final_status }}"
          echo "  - apply_conflicts_A: ${{ steps.patch_sync_parse.outputs.apply_conflicts_A }}"
          echo "  - apply_conflicts_B: ${{ steps.patch_sync_parse.outputs.apply_conflicts_B }}"
          echo "  - lock_update_blocked: ${{ steps.patch_sync_parse.outputs.lock_update_blocked }}"
          echo "  - force_update_lock: ${{ steps.patch_sync_parse.outputs.force_update_lock }}"
          echo ""
          
          # Warn å¤„ç†ï¼ˆCategory B ç­‰ï¼‰
          if [ "$SHOULD_WARN" == "true" ]; then
            echo "::warning::OpenMemory patch sync has warnings - $SUMMARY"
          fi
          
          # Fail å¤„ç†ï¼ˆCategory A æˆ– error çŠ¶æ€ï¼‰
          if [ "$SHOULD_FAIL" == "true" ]; then
            echo ""
            echo "::error::OpenMemory patch sync check failed"
            echo "::error::$SUMMARY"
            echo "::error::Conflict artifacts: ${{ steps.patch_sync_parse.outputs.conflict_artifacts_dir }}"
            exit 1
          fi
          
          echo ""
          echo "[OK] OpenMemory patch sync check passed"

      # ========================================================================
      # OpenMemory Upstream Drift Check (Allowed Failure)
      # - ä¿æŒ warning/éžé˜»å¡ž
      # - Security priority (exit=1): åœ¨ Summary ä¸­çªå‡ºæ˜¾ç¤ºï¼Œåˆ›å»ºé«˜ä¼˜å…ˆçº§ Issue
      # - å†»ç»“çŠ¶æ€ (exit=3): éœ€è¦äººå·¥ override
      # ========================================================================
      - name: Run OpenMemory upstream drift check
        id: openmemory_drift_check
        timeout-minutes: 2
        continue-on-error: true
        env:
          # äººå·¥ override çŽ¯å¢ƒå˜é‡ï¼ˆé€šè¿‡ workflow_dispatch è¾“å…¥ï¼‰
          OPENMEMORY_FREEZE_OVERRIDE: ${{ github.event.inputs.openmemory_freeze_override || 'false' }}
          OPENMEMORY_FREEZE_OVERRIDE_REASON: ${{ github.event.inputs.openmemory_freeze_override_reason || '' }}
        run: |
          echo "::group::OpenMemory Upstream Drift Check"
          
          # æ˜¾ç¤º override çŠ¶æ€
          if [ "$OPENMEMORY_FREEZE_OVERRIDE" == "true" ]; then
            echo "Freeze Override requested: $OPENMEMORY_FREEZE_OVERRIDE_REASON"
          fi
          
          # è¿è¡Œ drift check è„šæœ¬
          python scripts/openmemory_upstream_drift_check.py
          DRIFT_EXIT_CODE=$?
          echo "::endgroup::"
          
          # ä¿å­˜åŽŸå§‹é€€å‡ºç 
          echo "drift_exit_code=$DRIFT_EXIT_CODE" >> $GITHUB_OUTPUT
          
          # æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦
          if [ -f ".artifacts/openmemory-upstream-drift.json" ]; then
            echo "=== Drift Report Summary ==="
            cat .artifacts/openmemory-upstream-drift.json | head -40
          fi
          
          # ä½¿ç”¨ç»Ÿä¸€çš„è§£æžè„šæœ¬æå–å­—æ®µï¼ˆå‡å°‘ bash è§£æžè„†å¼±æ€§ï¼‰
          echo "::group::Parse Drift Report"
          python scripts/openmemory_drift_parse.py .artifacts/openmemory-upstream-drift.json --github-output $GITHUB_OUTPUT
          echo "::endgroup::"
          
          # æ ¹æ®é€€å‡ºç è¾“å‡ºè­¦å‘Š
          if [ $DRIFT_EXIT_CODE -eq 1 ]; then
            echo "::warning::OpenMemory upstream has security updates available!"
          elif [ $DRIFT_EXIT_CODE -eq 3 ]; then
            echo "::warning::OpenMemory upgrade is frozen and requires manual override!"
          fi
          
          # å§‹ç»ˆè¿”å›ž 0ï¼ˆcontinue-on-error ä»…å½±å“ step outcomeï¼Œä¸å½±å“ outputsï¼‰
          exit 0

      - name: Upload OpenMemory upstream drift report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: openmemory-upstream-drift-${{ github.run_number }}
          path: |
            .artifacts/openmemory-upstream-drift.json
            .artifacts/openmemory-security-summary.md
          retention-days: 30
          if-no-files-found: ignore

      # ========================================================================
      # Security Update Notification (å½“æ£€æµ‹åˆ°å®‰å…¨æ›´æ–°æ—¶åˆ›å»ºé«˜ä¼˜å…ˆçº§ Issue)
      # ========================================================================
      - name: Create security update issue
        if: ${{ steps.openmemory_drift_check.outputs.has_security_update == 'true' }}
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // å°è¯•è¯»å– security summary æ–‡ä»¶
            let securitySummary = '';
            try {
              securitySummary = fs.readFileSync('.artifacts/openmemory-security-summary.md', 'utf8');
            } catch (e) {
              console.log('Security summary file not found, using default content');
            }
            
            const title = 'ðŸš¨ [Security/High] OpenMemory ä¸Šæ¸¸å®‰å…¨æ›´æ–°æ£€æµ‹';
            const body = securitySummary || `## Security Update Detected

            Nightly build #${{ github.run_number }} æ£€æµ‹åˆ° OpenMemory ä¸Šæ¸¸å­˜åœ¨å®‰å…¨æ›´æ–°ã€‚

            ### è¯¦æƒ…
            - **æ£€æµ‹æ—¶é—´**: ${new Date().toISOString()}
            - **Workflow Run**: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            - **Priority**: ${{ steps.openmemory_drift_check.outputs.priority }}
            - **Frozen**: ${{ steps.openmemory_drift_check.outputs.is_frozen }}

            ### å»ºè®®æ“ä½œ
            1. æŸ¥çœ‹ [drift report artifact](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
            2. è¯„ä¼°å®‰å…¨æ›´æ–°å†…å®¹
            3. æŒ‰ç…§ \`make openmemory-upgrade-check\` æµç¨‹è¿›è¡Œå‡çº§

            ---
            *æ­¤ Issue ç”± Nightly CI è‡ªåŠ¨åˆ›å»ºï¼Œæ ‡è®°ä¸ºé«˜ä¼˜å…ˆçº§*
            `;

            // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ ‡é¢˜çš„ open issue
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'security,openmemory'
            });

            // æŸ¥æ‰¾ä»»ä½• security+openmemory çš„ open issue
            const existingIssue = issues.data.find(i => 
              i.title.includes('[Security') && i.title.includes('OpenMemory')
            );
            
            if (existingIssue) {
              console.log(`Issue already exists: #${existingIssue.number}`);
              // æ·»åŠ è¯„è®ºæ›´æ–°
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: existingIssue.number,
                body: `ðŸš¨ **å†æ¬¡æ£€æµ‹åˆ°å®‰å…¨æ›´æ–°** (Run #${{ github.run_number }})\n\n` +
                      `- Workflow: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n` +
                      `- Priority: ${{ steps.openmemory_drift_check.outputs.priority }}\n\n` +
                      `è¯·å°½å¿«å¤„ç†æ­¤å®‰å…¨æ›´æ–°ï¼`
              });
              
              // ç¡®ä¿é«˜ä¼˜å…ˆçº§æ ‡ç­¾å­˜åœ¨
              try {
                await github.rest.issues.addLabels({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: existingIssue.number,
                  labels: ['priority:high']
                });
              } catch (e) {
                console.log('Could not add priority:high label:', e.message);
              }
            } else {
              // åˆ›å»ºæ–° issueï¼ˆå¸¦é«˜ä¼˜å…ˆçº§æ ‡ç­¾ï¼‰
              const newIssue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['security', 'openmemory', 'nightly-ci', 'priority:high']
              });
              console.log(`Created new issue: #${newIssue.data.number}`);
            }
        continue-on-error: true

      # ========================================================================
      # Freeze Override Required Notification (å½“éœ€è¦äººå·¥ override æ—¶)
      # ========================================================================
      - name: Notify freeze override required
        if: ${{ steps.openmemory_drift_check.outputs.needs_override == 'true' }}
        run: |
          echo "::warning::OpenMemory å‡çº§å¤„äºŽå†»ç»“çŠ¶æ€ï¼Œå¦‚éœ€æ£€æŸ¥ä¸Šæ¸¸æ›´æ–°ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‚æ•°é‡æ–°è¿è¡Œ workflow:"
          echo ""
          echo "  workflow_dispatch inputs:"
          echo "    openmemory_freeze_override: true"
          echo "    openmemory_freeze_override_reason: 'å…·ä½“åŽŸå› '"
          echo ""
          echo "æˆ–é€šè¿‡ gh CLI:"
          echo "  gh workflow run nightly.yml -f openmemory_freeze_override=true -f openmemory_freeze_override_reason='reason'"

      - name: Upload verification results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: nightly-verification-results
          path: .artifacts/unified-stack/verify-results.json
          retention-days: 14
          if-no-files-found: ignore

      - name: Run Gateway integration tests
        id: gateway_integration
        timeout-minutes: 10
        env:
          # Full å±‚: ä¸å¯ç”¨ HTTP_ONLYï¼Œæ‰§è¡Œå®Œæ•´æµ‹è¯•ï¼ˆå«é™çº§ï¼‰
          HTTP_ONLY_MODE: "0"
          SKIP_DEGRADATION_TEST: "0"
        run: |
          echo "::group::Gateway Integration Tests (Full Mode)"
          make test-gateway-integration 2>&1 | tee /tmp/gateway-integration-output.txt
          echo "::endgroup::"

      - name: Run Step3 PGVector integration tests
        id: step3_pgvector
        if: ${{ github.event_name == 'schedule' || github.event.inputs.run_step3_pgvector_tests == 'true' }}
        timeout-minutes: 15
        env:
          TEST_PGVECTOR_DSN: postgresql://postgres:postgres@localhost:5432/${{ env.POSTGRES_DB }}
        run: |
          echo "::group::Step3 PGVector Integration Tests"
          pip install -q -r apps/step3_seekdb_rag_hybrid/requirements.dev.txt
          make test-step3-pgvector 2>&1 | tee /tmp/step3-pgvector-output.txt
          echo "::endgroup::"

      - name: Upload Step3 PGVector test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: nightly-step3-pgvector-results
          path: |
            .artifacts/test-results/step3-pgvector.xml
          retention-days: 14
          if-no-files-found: ignore

      # ========================================================================
      # Step3 Collection Migrate (dry-run) - éªŒè¯è¿ç§»è„šæœ¬
      # ========================================================================
      - name: Run Step3 Collection Migrate (dry-run)
        id: step3_migrate
        if: ${{ github.event_name == 'schedule' || github.event.inputs.run_step3_migration_rehearsal == 'true' }}
        timeout-minutes: 10
        env:
          # ä¼˜å…ˆä½¿ç”¨ STEP3_PGVECTOR_DSNï¼ˆå‡å°‘æŽ¨æ–­ï¼Œæ˜Žç¡®æŒ‡å®šï¼‰
          STEP3_PGVECTOR_DSN: postgresql://postgres:${{ env.POSTGRES_PASSWORD }}@localhost:5432/${{ env.POSTGRES_DB }}
          # ä½¿ç”¨ STEP3_PG_* å‰ç¼€çŽ¯å¢ƒå˜é‡ï¼ˆä¸Ž pgvector_collection_migrate.py ä¼˜å…ˆçº§ä¸€è‡´ï¼‰
          STEP3_PG_HOST: localhost
          STEP3_PG_PORT: "5432"
          STEP3_PG_DB: ${{ env.POSTGRES_DB }}
          STEP3_PG_USER: postgres
          STEP3_PG_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
          # ä½¿ç”¨éš”ç¦»æµ‹è¯•è¡¨ï¼Œé¿å…å½±å“å®žé™…æ•°æ®
          STEP3_PG_SCHEMA: step3_test
          STEP3_PG_TABLE: chunks_test
        run: |
          echo "::group::Step3 Collection Migrate (dry-run)"
          
          # ä½¿ç”¨ Makefile ç›®æ ‡æ‰§è¡Œ dry-run
          make step3-migrate-dry-run
          
          echo "::endgroup::"
          
          echo "::group::Step3 Migrate artifacts"
          ls -la .artifacts/step3-migrate/ || true
          echo "::endgroup::"

      - name: Upload Step3 Migrate results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: nightly-step3-migrate-results
          path: |
            .artifacts/step3-migrate/*.json
          retention-days: 14
          if-no-files-found: ignore

      # ========================================================================
      # Step3 Smoke Test (Full Mode) - ç´¢å¼•/æ£€ç´¢/ä¸€è‡´æ€§æ£€æŸ¥
      # ========================================================================
      - name: Run Step3 Smoke Test
        id: step3_smoke
        if: ${{ github.event_name == 'schedule' || github.event.inputs.run_step3_smoke_with_check == 'true' }}
        timeout-minutes: 10
        env:
          # ä¼˜å…ˆä½¿ç”¨ STEP3_PGVECTOR_DSNï¼ˆå‡å°‘æŽ¨æ–­ï¼Œæ˜Žç¡®æŒ‡å®šï¼‰
          STEP3_PGVECTOR_DSN: postgresql://postgres:${{ env.POSTGRES_PASSWORD }}@localhost:5432/${{ env.POSTGRES_DB }}
          TEST_PGVECTOR_DSN: postgresql://postgres:postgres@localhost:5432/${{ env.POSTGRES_DB }}
          # Full å±‚: ä¸è·³è¿‡ä¸€è‡´æ€§æ£€æŸ¥ï¼Œæ‰§è¡Œå®Œæ•´éªŒè¯
          STEP3_SKIP_CHECK: "0"
          # Nightly: å¯ç”¨ SHA256 æ ¡éªŒï¼ˆæ£€æµ‹æ•°æ®ä¸€è‡´æ€§é—®é¢˜ï¼‰
          STEP3_INDEX_VERIFY_SHA256: "1"
          # å¯é€šè¿‡ä»¥ä¸‹å˜é‡è°ƒæ•´é‡‡æ ·å¤§å°ï¼ˆé¢„ç®—å—é™æ—¶å‡å°ï¼‰
          STEP3_SMOKE_INDEX_SAMPLE_SIZE: "30"
          STEP3_SMOKE_LIMIT: "50"
        run: |
          echo "::group::Step3 Smoke Test (Full Mode)"
          mkdir -p .artifacts/step3-smoke
          make step3-run-smoke 2>&1 | tee .artifacts/step3-smoke/smoke-output.txt
          echo "::endgroup::"

      - name: Collect Step3 Smoke artifacts
        if: always()
        run: |
          mkdir -p .artifacts/step3-smoke
          # æ”¶é›† /tmp ä¸‹çš„ smoke JSON æ–‡ä»¶
          cp /tmp/step3_smoke_*.json .artifacts/step3-smoke/ 2>/dev/null || true
          # è®°å½•æ—¶é—´æˆ³
          echo "Timestamp: $(date -u +"%Y-%m-%dT%H:%M:%SZ")" > .artifacts/step3-smoke/metadata.txt

      - name: Upload Step3 Smoke results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: nightly-step3-smoke-results
          path: |
            .artifacts/step3-smoke/
            /tmp/step3_smoke_*.json
          retention-days: 14
          if-no-files-found: ignore

      # ========================================================================
      # Step3 Nightly Rebuild - æ ‡å‡†åŒ–æµç¨‹ï¼ˆfull rebuild + gate + activateï¼‰
      # ========================================================================
      # Nightly å¼ºæ‰§è¡Œæµç¨‹ï¼š
      #   1. ä¿å­˜å½“å‰ active collectionï¼ˆç”¨äºŽå›žæ»šï¼‰
      #   2. full rebuild ç”Ÿæˆå¸¦ version_tag çš„æ–° collectionï¼ˆä¸è¦†ç›–æ—§ï¼‰
      #   3. æ‰§è¡Œ seek_query --query-set nightly_default é—¨ç¦æ£€æŸ¥
      #   4. é—¨ç¦é€šè¿‡åŽè°ƒç”¨ set_active_collection æ¿€æ´»
      #   5. å¤±è´¥è¾“å‡ºæ˜Žç¡®å›žæ»šæŒ‡ä»¤ï¼ˆå›žåˆ°æ—§ active_collectionï¼‰
      # ========================================================================
      - name: Run Step3 Nightly Rebuild
        id: step3_nightly_rebuild
        if: ${{ github.event_name == 'schedule' || github.event.inputs.run_nightly_rebuild == 'true' }}
        timeout-minutes: 30
        env:
          STEP3_PGVECTOR_DSN: postgresql://postgres:${{ env.POSTGRES_PASSWORD }}@localhost:5432/${{ env.POSTGRES_DB }}
          STEP3_INDEX_BACKEND: pgvector
          STEP3_PG_SCHEMA: step3
          STEP3_PG_TABLE: chunks
          STEP3_PGVECTOR_COLLECTION_STRATEGY: single_table
          STEP3_PGVECTOR_AUTO_INIT: "1"
          # Nightly Rebuild å¯ç”¨ SHA256 æ ¡éªŒ
          STEP3_INDEX_VERIFY_SHA256: "1"
          # é—¨ç¦é…ç½®
          STEP3_NIGHTLY_QUERY_SET: nightly_default
          STEP3_NIGHTLY_MIN_OVERLAP: "0.5"
          STEP3_NIGHTLY_TOP_K: "10"
          # å…è®¸ Nightly å®žé™…åˆ‡æ¢ active collectionï¼ˆä»… Nightly å¯è®¾ç½®ï¼‰
          STEP3_ALLOW_ACTIVE_COLLECTION_SWITCH: "1"
        run: |
          echo "::group::Step3 Nightly Rebuild"
          mkdir -p .artifacts/step3-nightly-rebuild
          
          # æ‰§è¡Œ Nightly Rebuild æµç¨‹
          make step3-nightly-rebuild JSON_OUTPUT=1 2>&1 | tee .artifacts/step3-nightly-rebuild/nightly-rebuild-output.txt
          REBUILD_EXIT_CODE=${PIPESTATUS[0]}
          
          echo "::endgroup::"
          
          # è§£æžç»“æžœ
          if [ -f ".artifacts/step3-nightly-rebuild/nightly-rebuild.json" ]; then
            echo "::group::Nightly Rebuild Result"
            cat .artifacts/step3-nightly-rebuild/nightly-rebuild.json
            echo "::endgroup::"
            
            # æå–å…³é”®ä¿¡æ¯
            REBUILD_SUCCESS=$(python3 -c "import json; d=json.load(open('.artifacts/step3-nightly-rebuild/nightly-rebuild.json')); print('true' if d.get('success') else 'false')" 2>/dev/null || echo "false")
            NEW_COLLECTION=$(python3 -c "import json; d=json.load(open('.artifacts/step3-nightly-rebuild/nightly-rebuild.json')); print(d.get('collection',{}).get('new','unknown'))" 2>/dev/null || echo "unknown")
            OLD_COLLECTION=$(python3 -c "import json; d=json.load(open('.artifacts/step3-nightly-rebuild/nightly-rebuild.json')); print(d.get('collection',{}).get('old','none'))" 2>/dev/null || echo "none")
            GATE_PASSED=$(python3 -c "import json; d=json.load(open('.artifacts/step3-nightly-rebuild/nightly-rebuild.json')); print('true' if d.get('gate',{}).get('passed') else 'false')" 2>/dev/null || echo "false")
            GATE_PROFILE=$(python3 -c "import json; d=json.load(open('.artifacts/step3-nightly-rebuild/nightly-rebuild.json')); print(d.get('gate',{}).get('profile','unknown'))" 2>/dev/null || echo "unknown")
            GATE_VERSION=$(python3 -c "import json; d=json.load(open('.artifacts/step3-nightly-rebuild/nightly-rebuild.json')); print(d.get('gate',{}).get('version','unknown'))" 2>/dev/null || echo "unknown")
            ROLLBACK_CMD=$(python3 -c "import json; d=json.load(open('.artifacts/step3-nightly-rebuild/nightly-rebuild.json')); print(d.get('rollback',{}).get('command',''))" 2>/dev/null || echo "")
            
            echo "rebuild_success=$REBUILD_SUCCESS" >> $GITHUB_OUTPUT
            echo "new_collection=$NEW_COLLECTION" >> $GITHUB_OUTPUT
            echo "old_collection=$OLD_COLLECTION" >> $GITHUB_OUTPUT
            echo "gate_passed=$GATE_PASSED" >> $GITHUB_OUTPUT
            echo "gate_profile=$GATE_PROFILE" >> $GITHUB_OUTPUT
            echo "gate_version=$GATE_VERSION" >> $GITHUB_OUTPUT
            
            # å†™å…¥ metadata.json åŒ…å« gate profile/version ä¿¡æ¯
            GIT_SHA=$(git rev-parse --short HEAD 2>/dev/null || echo "unknown")
            TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
            python3 -c "
          import json
          data = {
              'timestamp': '$TIMESTAMP',
              'git_sha': '$GIT_SHA',
              'workflow_run': '${{ github.run_number }}',
              'rebuild_success': $REBUILD_SUCCESS,
              'collection': {
                  'old': '$OLD_COLLECTION',
                  'new': '$NEW_COLLECTION'
              },
              'gate': {
                  'passed': $GATE_PASSED,
                  'profile': '$GATE_PROFILE',
                  'version': '$GATE_VERSION'
              },
              'env': {
                  'STEP3_ALLOW_ACTIVE_COLLECTION_SWITCH': '1',
                  'STEP3_NIGHTLY_QUERY_SET': '$STEP3_NIGHTLY_QUERY_SET',
                  'STEP3_NIGHTLY_MIN_OVERLAP': '$STEP3_NIGHTLY_MIN_OVERLAP',
                  'STEP3_NIGHTLY_TOP_K': '$STEP3_NIGHTLY_TOP_K'
              }
          }
          with open('.artifacts/step3-nightly-rebuild/metadata.json', 'w') as f:
              json.dump(data, f, indent=2)
          "
            
            if [ "$REBUILD_SUCCESS" != "true" ]; then
              echo "::error::Step3 Nightly Rebuild å¤±è´¥"
              if [ -n "$ROLLBACK_CMD" ]; then
                echo "::warning::å›žæ»šæŒ‡ä»¤: $ROLLBACK_CMD"
              fi
            else
              echo "::notice::Step3 Nightly Rebuild æˆåŠŸ: $NEW_COLLECTION (gate profile: $GATE_PROFILE, version: $GATE_VERSION)"
            fi
          fi
          
          exit $REBUILD_EXIT_CODE

      - name: Upload Step3 Nightly Rebuild results
        if: always() && (github.event_name == 'schedule' || github.event.inputs.run_nightly_rebuild == 'true')
        uses: actions/upload-artifact@v4
        with:
          name: nightly-step3-rebuild-results-${{ github.run_number }}
          path: |
            .artifacts/step3-nightly-rebuild/
          retention-days: 14
          if-no-files-found: ignore

      # ========================================================================
      # Step3 Dual-Read æ¯”è¾ƒæµ‹è¯• - éªŒè¯ primary/shadow åŽç«¯ä¸€è‡´æ€§
      # ========================================================================
      - name: Run Step3 Dual-Read Test
        id: step3_dual_read
        if: ${{ github.event_name == 'schedule' || github.event.inputs.run_dual_read_test == 'true' }}
        timeout-minutes: 10
        env:
          STEP3_PGVECTOR_DSN: postgresql://postgres:${{ env.POSTGRES_PASSWORD }}@localhost:5432/${{ env.POSTGRES_DB }}
          STEP3_INDEX_BACKEND: pgvector
          STEP3_PG_SCHEMA: step3
          STEP3_PG_TABLE: chunks
          STEP3_PGVECTOR_COLLECTION_STRATEGY: single_table
          STEP3_PGVECTOR_AUTO_INIT: "1"
          # Shadow åŽç«¯é…ç½®
          STEP3_PGVECTOR_SHADOW_STRATEGY: per_table
          STEP3_PGVECTOR_SHADOW_TABLE: chunks_shadow
        run: |
          echo "::group::Step3 Dual-Read Test"
          mkdir -p .artifacts/dual-read
          
          # èŽ·å– git SHAï¼ˆç”¨äºŽé˜ˆå€¼ç‰ˆæœ¬è¿½è¸ªï¼‰
          GIT_SHA=$(git rev-parse --short HEAD 2>/dev/null || echo "unknown")
          THRESHOLD_VERSION="$(date -u +"%Y%m%dT%H%M%S")-${GIT_SHA}"
          
          # ä½¿ç”¨å†…ç½®æŸ¥è¯¢é›†æ‰§è¡ŒåŒè¯»æ¯”è¾ƒæµ‹è¯•
          # seek_query è¾“å‡ºçŽ°åœ¨åŒ…å«èšåˆ gate æ‘˜è¦ï¼ˆfail/warn/pass æ•°ã€æœ€å·® recommendationã€è§¦å‘é¡¹ç»Ÿè®¡ï¼‰
          # æŸ¥è¯¢åˆ—è¡¨å®šä¹‰åœ¨ seek_query.py çš„ BUILTIN_QUERY_SETS["nightly_default"]
          cd apps/step3_seekdb_rag_hybrid
          python -m seek_query \
            --query-set nightly_default \
            --dual-read \
            --dual-read-min-overlap 0.5 \
            --dual-read-report \
            --top-k 10 \
            --json 2>&1 | tee ../../.artifacts/dual-read/dual-read-results.json
          
          DUAL_READ_EXIT_CODE=${PIPESTATUS[0]}
          
          # è§£æž seek_query è¾“å‡ºï¼Œæå–èšåˆ gate æ‘˜è¦
          cd ../..
          GATE_SUMMARY=$(python3 -c "import json; f=open('.artifacts/dual-read/dual-read-results.json','r'); data=json.load(f); gate=data.get('aggregate_gate',{}); summary={'passed':gate.get('passed',True),'fail_count':gate.get('fail_count',0),'warn_count':gate.get('warn_count',0),'pass_count':gate.get('pass_count',0),'error_count':gate.get('error_count',0),'worst_recommendation':gate.get('worst_recommendation','safe_to_switch'),'violation_summary':gate.get('violation_summary',None)}; print(json.dumps(summary))" 2>/dev/null || echo '{"passed": true, "error": "parse_failed"}')
          
          # æå–é˜ˆå€¼æ¥æºä¿¡æ¯
          THRESHOLDS_SOURCE=$(python3 -c "import json; f=open('.artifacts/dual-read/dual-read-results.json','r'); data=json.load(f); tm=data.get('thresholds_metadata',{}); si={}; ct=tm.get('compare_thresholds',{}); src=ct.get('source',{}); si={'primary_source':src.get('primary_source','default'),'env_keys_used':src.get('env_keys_used',[]),'cli_overrides':src.get('cli_overrides',[])}; gt=tm.get('gate_thresholds',{}); si['gate_source']=gt.get('source','cli') if gt else 'cli'; print(json.dumps(si))" 2>/dev/null || echo '{}')
          
          # è®°å½•æµ‹è¯•å…ƒæ•°æ®ï¼ˆå«é˜ˆå€¼æ¥æºå’Œç‰ˆæœ¬ï¼‰
          cat > .artifacts/dual-read/metadata.json << 'METADATA_EOF'
          {
            "timestamp": "TIMESTAMP_PLACEHOLDER",
            "query_set": "nightly_default",
            "exit_code": EXIT_CODE_PLACEHOLDER,
            "primary_strategy": "single_table",
            "shadow_strategy": "per_table",
            "shadow_table": "chunks_shadow",
            "gate_summary": GATE_SUMMARY_PLACEHOLDER,
            "thresholds": {
              "version": "THRESHOLD_VERSION_PLACEHOLDER",
              "git_sha": "GIT_SHA_PLACEHOLDER",
              "source": THRESHOLDS_SOURCE_PLACEHOLDER,
              "cli_args": {
                "dual_read_min_overlap": 0.5,
                "top_k": 10
              }
            }
          }
          METADATA_EOF
          
          # æ›¿æ¢å ä½ç¬¦
          TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          sed -i "s|TIMESTAMP_PLACEHOLDER|$TIMESTAMP|g" .artifacts/dual-read/metadata.json
          sed -i "s|EXIT_CODE_PLACEHOLDER|$DUAL_READ_EXIT_CODE|g" .artifacts/dual-read/metadata.json
          sed -i "s|GATE_SUMMARY_PLACEHOLDER|$GATE_SUMMARY|g" .artifacts/dual-read/metadata.json
          sed -i "s|THRESHOLD_VERSION_PLACEHOLDER|$THRESHOLD_VERSION|g" .artifacts/dual-read/metadata.json
          sed -i "s|GIT_SHA_PLACEHOLDER|$GIT_SHA|g" .artifacts/dual-read/metadata.json
          sed -i "s|THRESHOLDS_SOURCE_PLACEHOLDER|$THRESHOLDS_SOURCE|g" .artifacts/dual-read/metadata.json
          
          echo "::endgroup::"
          
          # æ˜¾ç¤º gate æ‘˜è¦åˆ° CI æ—¥å¿—
          echo "::group::Dual-Read Gate Summary"
          echo "Gate Summary: $GATE_SUMMARY"
          echo "Thresholds Source: $THRESHOLDS_SOURCE"
          echo "Thresholds Version: $THRESHOLD_VERSION"
          echo "::endgroup::"
          
          if [ $DUAL_READ_EXIT_CODE -ne 0 ]; then
            echo "::warning::Dual-read test completed with issues (exit_code=$DUAL_READ_EXIT_CODE)"
          fi
          
          exit $DUAL_READ_EXIT_CODE

      - name: Upload Dual-Read results
        if: always() && (github.event_name == 'schedule' || github.event.inputs.run_dual_read_test == 'true')
        uses: actions/upload-artifact@v4
        with:
          name: nightly-dual-read-results-${{ github.run_number }}
          path: .artifacts/dual-read/
          retention-days: 14
          if-no-files-found: ignore

      # ========================================================================
      # Step3 PGVector è¿ç§»æ¼”ç»ƒé›†æˆæµ‹è¯•
      # ========================================================================
      - name: Run Step3 PGVector Migration Drill Test
        id: step3_migration_drill
        if: ${{ github.event_name == 'schedule' || github.event.inputs.run_step3_migration_drill == 'true' }}
        timeout-minutes: 15
        env:
          TEST_PGVECTOR_DSN: postgresql://postgres:${{ env.POSTGRES_PASSWORD }}@localhost:5432/${{ env.POSTGRES_DB }}
        run: |
          echo "::group::Step3 PGVector Migration Drill Test"
          pip install -q -r apps/step3_seekdb_rag_hybrid/requirements.dev.txt
          make test-step3-pgvector-migration-drill 2>&1 | tee /tmp/step3-migration-drill-output.txt
          echo "::endgroup::"

      - name: Collect Step3 Migration Drill diagnostics on failure
        if: failure() && steps.step3_migration_drill.outcome == 'failure'
        run: |
          mkdir -p .artifacts/step3-diagnostics
          echo "=== Step3 Migration Drill Diagnostics ===" > .artifacts/step3-diagnostics/migration-drill-diagnostics.txt
          echo "Timestamp: $(date -u +"%Y-%m-%dT%H:%M:%SZ")" >> .artifacts/step3-diagnostics/migration-drill-diagnostics.txt
          echo "" >> .artifacts/step3-diagnostics/migration-drill-diagnostics.txt
          
          # PostgreSQL è¯Šæ–­ä¿¡æ¯
          echo "=== pg_extension ===" >> .artifacts/step3-diagnostics/migration-drill-diagnostics.txt
          docker compose -p engram -f docker-compose.unified.yml exec -T postgres \
            psql -U postgres -d engram -c "SELECT extname, extversion FROM pg_extension;" \
            >> .artifacts/step3-diagnostics/migration-drill-diagnostics.txt 2>&1 || true
          
          echo "" >> .artifacts/step3-diagnostics/migration-drill-diagnostics.txt
          echo "=== Schemas (\\dn) ===" >> .artifacts/step3-diagnostics/migration-drill-diagnostics.txt
          docker compose -p engram -f docker-compose.unified.yml exec -T postgres \
            psql -U postgres -d engram -c "\dn" \
            >> .artifacts/step3-diagnostics/migration-drill-diagnostics.txt 2>&1 || true
          
          echo "" >> .artifacts/step3-diagnostics/migration-drill-diagnostics.txt
          echo "=== Tables (\\dt step3_test.*) ===" >> .artifacts/step3-diagnostics/migration-drill-diagnostics.txt
          docker compose -p engram -f docker-compose.unified.yml exec -T postgres \
            psql -U postgres -d engram -c "\dt step3_test.*" \
            >> .artifacts/step3-diagnostics/migration-drill-diagnostics.txt 2>&1 || true
          
          echo "" >> .artifacts/step3-diagnostics/migration-drill-diagnostics.txt
          echo "=== Indexes (\\di step3_test.*) ===" >> .artifacts/step3-diagnostics/migration-drill-diagnostics.txt
          docker compose -p engram -f docker-compose.unified.yml exec -T postgres \
            psql -U postgres -d engram -c "\di step3_test.*" \
            >> .artifacts/step3-diagnostics/migration-drill-diagnostics.txt 2>&1 || true
          
          # æ”¶é›†æµ‹è¯•è¾“å‡º
          cp /tmp/step3-migration-drill-output.txt .artifacts/step3-diagnostics/ 2>/dev/null || true

      - name: Upload Step3 Migration Drill results
        if: always() && (github.event_name == 'schedule' || github.event.inputs.run_step3_migration_drill == 'true')
        uses: actions/upload-artifact@v4
        with:
          name: nightly-step3-migration-drill-results-${{ github.run_number }}
          path: |
            .artifacts/test-results/step3-pgvector-migration-drill.xml
            .artifacts/step3-diagnostics/migration-drill-diagnostics.txt
          retention-days: 14
          if-no-files-found: ignore

      - name: Collect Unified Stack logs on failure
        if: failure() && (steps.verify_unified.outcome == 'failure' || steps.gateway_integration.outcome == 'failure' || steps.step3_pgvector.outcome == 'failure' || steps.step3_migrate.outcome == 'failure' || steps.step3_smoke.outcome == 'failure' || steps.step3_nightly_rebuild.outcome == 'failure' || steps.step3_dual_read.outcome == 'failure' || steps.step3_migration_drill.outcome == 'failure')
        env:
          GATEWAY_URL: http://localhost:8787
          OPENMEMORY_URL: http://localhost:8080
        run: |
          mkdir -p .artifacts/unified-stack
          cp /tmp/verify-unified-output.txt .artifacts/unified-stack/ 2>/dev/null || true
          cp /tmp/gateway-integration-output.txt .artifacts/unified-stack/ 2>/dev/null || true
          cp /tmp/step3-pgvector-output.txt .artifacts/unified-stack/ 2>/dev/null || true
          cp .artifacts/test-results/step3-pgvector.xml .artifacts/unified-stack/ 2>/dev/null || true
          # æ”¶é›† Step3 Migrate æ—¥å¿—
          cp -r .artifacts/step3-migrate .artifacts/unified-stack/ 2>/dev/null || true
          # æ”¶é›† compose é…ç½®å’Œæ—¥å¿—ï¼ˆç»Ÿä¸€ä½ç½®ï¼‰
          docker compose -p engram -f docker-compose.unified.yml config > .artifacts/compose-config.yml 2>&1 || true
          docker compose -p engram -f docker-compose.unified.yml ps > .artifacts/compose-ps.txt 2>&1 || true
          docker compose -p engram -f docker-compose.unified.yml logs --no-color > .artifacts/compose-logs.txt 2>&1 || true
          # åŒæ—¶ä¿ç•™åœ¨ unified-stack å­ç›®å½•ï¼ˆå‘åŽå…¼å®¹ï¼‰
          cp .artifacts/compose-config.yml .artifacts/unified-stack/ 2>/dev/null || true
          cp .artifacts/compose-ps.txt .artifacts/unified-stack/ 2>/dev/null || true
          cp .artifacts/compose-logs.txt .artifacts/unified-stack/ 2>/dev/null || true
          # Health check outputs
          echo "=== Gateway Health Check ===" > .artifacts/unified-stack/health-checks.txt
          curl -v ${GATEWAY_URL}/health >> .artifacts/unified-stack/health-checks.txt 2>&1 || true
          echo "" >> .artifacts/unified-stack/health-checks.txt
          echo "=== OpenMemory Health Check ===" >> .artifacts/unified-stack/health-checks.txt
          curl -v ${OPENMEMORY_URL}/health >> .artifacts/unified-stack/health-checks.txt 2>&1 || true
          # Collect pytest reports
          cp -r apps/step2_openmemory_gateway/gateway/.pytest_cache .artifacts/unified-stack/ 2>/dev/null || true
          cp apps/step2_openmemory_gateway/gateway/pytest-report.* .artifacts/unified-stack/ 2>/dev/null || true
          # æ±‡æ€» openmemory-sync æŠ¥å‘Š
          echo '{"check":' > .artifacts/openmemory-sync-report.json
          cat .artifacts/openmemory-sync-check-report.json 2>/dev/null || echo 'null' >> .artifacts/openmemory-sync-report.json
          echo ',"apply":' >> .artifacts/openmemory-sync-report.json
          cat .artifacts/openmemory-sync-apply-report.json 2>/dev/null || echo 'null' >> .artifacts/openmemory-sync-report.json
          echo ',"verify":' >> .artifacts/openmemory-sync-report.json
          cat .artifacts/openmemory-sync-verify-report.json 2>/dev/null || echo 'null' >> .artifacts/openmemory-sync-report.json
          echo '}' >> .artifacts/openmemory-sync-report.json

      - name: Collect Step3 diagnostics on failure
        if: failure() && (steps.step3_pgvector.outcome == 'failure' || steps.step3_migrate.outcome == 'failure' || steps.step3_smoke.outcome == 'failure' || steps.step3_nightly_rebuild.outcome == 'failure' || steps.step3_dual_read.outcome == 'failure' || steps.step3_migration_drill.outcome == 'failure')
        run: |
          mkdir -p .artifacts/step3-diagnostics
          echo "=== Step3 Diagnostics ===" > .artifacts/step3-diagnostics/diagnostics.txt
          echo "Timestamp: $(date -u +"%Y-%m-%dT%H:%M:%SZ")" >> .artifacts/step3-diagnostics/diagnostics.txt
          echo "" >> .artifacts/step3-diagnostics/diagnostics.txt
          
          # PostgreSQL è¯Šæ–­ä¿¡æ¯
          echo "=== pg_extension ===" >> .artifacts/step3-diagnostics/diagnostics.txt
          docker compose -p engram -f docker-compose.unified.yml exec -T postgres \
            psql -U postgres -d engram -c "SELECT extname, extversion FROM pg_extension;" \
            >> .artifacts/step3-diagnostics/diagnostics.txt 2>&1 || true
          
          echo "" >> .artifacts/step3-diagnostics/diagnostics.txt
          echo "=== Schemas (\\dn) ===" >> .artifacts/step3-diagnostics/diagnostics.txt
          docker compose -p engram -f docker-compose.unified.yml exec -T postgres \
            psql -U postgres -d engram -c "\dn" \
            >> .artifacts/step3-diagnostics/diagnostics.txt 2>&1 || true
          
          echo "" >> .artifacts/step3-diagnostics/diagnostics.txt
          echo "=== Tables (\\dt step3.*) ===" >> .artifacts/step3-diagnostics/diagnostics.txt
          docker compose -p engram -f docker-compose.unified.yml exec -T postgres \
            psql -U postgres -d engram -c "\dt step3.*" \
            >> .artifacts/step3-diagnostics/diagnostics.txt 2>&1 || true
          
          echo "" >> .artifacts/step3-diagnostics/diagnostics.txt
          echo "=== Indexes (\\di step3.*) ===" >> .artifacts/step3-diagnostics/diagnostics.txt
          docker compose -p engram -f docker-compose.unified.yml exec -T postgres \
            psql -U postgres -d engram -c "\di step3.*" \
            >> .artifacts/step3-diagnostics/diagnostics.txt 2>&1 || true
          
          # æ”¶é›† Step3 smoke JSON æ–‡ä»¶
          cp /tmp/step3_smoke_*.json .artifacts/step3-diagnostics/ 2>/dev/null || true
          
          # æ”¶é›† Step3 è¿ç§» dry-run JSONï¼ˆå¦‚å­˜åœ¨ï¼‰
          cp .artifacts/step3-migrate/*.json .artifacts/step3-diagnostics/ 2>/dev/null || true
          
          # æ”¶é›† Step3 Nightly Rebuild ç»“æžœï¼ˆå¦‚å­˜åœ¨ï¼‰
          cp -r .artifacts/step3-nightly-rebuild .artifacts/step3-diagnostics/ 2>/dev/null || true

      - name: Upload Step3 diagnostics on failure
        if: failure() && (steps.step3_pgvector.outcome == 'failure' || steps.step3_migrate.outcome == 'failure' || steps.step3_smoke.outcome == 'failure' || steps.step3_nightly_rebuild.outcome == 'failure' || steps.step3_dual_read.outcome == 'failure')
        uses: actions/upload-artifact@v4
        with:
          name: step3-diagnostics-${{ github.run_number }}
          path: .artifacts/step3-diagnostics/
          retention-days: 14
          if-no-files-found: ignore

      # ========================================================================
      # Step 5: Docker Build Verification (Optional via input)
      # ========================================================================
      - name: Docker Build Verification
        id: verify_build
        if: ${{ github.event_name == 'schedule' || github.event.inputs.run_build == 'true' }}
        timeout-minutes: 20
        run: |
          echo "::group::Static Build Checks"
          make verify-build-static 2>&1 | tee /tmp/verify-build-static-output.txt
          echo "::endgroup::"
          
          echo "::group::Full Docker Build"
          make verify-build 2>&1 | tee /tmp/verify-build-output.txt
          echo "::endgroup::"
          
          echo "::group::Built Images"
          docker images | grep engram || true
          echo "::endgroup::"

      - name: Collect Build logs on failure
        if: failure() && steps.verify_build.outcome == 'failure'
        run: |
          mkdir -p .artifacts/build
          cp /tmp/verify-build-static-output.txt .artifacts/build/ 2>/dev/null || true
          cp /tmp/verify-build-output.txt .artifacts/build/ 2>/dev/null || true
          docker compose -p engram -f docker-compose.unified.yml config > .artifacts/build/compose-config.yml 2>&1 || true
          docker compose -p engram -f docker-compose.unified.yml ps > .artifacts/build/compose-ps.txt 2>&1 || true
          docker compose -p engram -f docker-compose.unified.yml logs --no-color > .artifacts/build/compose-logs.txt 2>&1 || true

      # ========================================================================
      # Final: Upload all artifacts on failure
      # ========================================================================
      - name: Collect Step3 smoke JSON on failure (final)
        if: failure()
        run: |
          mkdir -p .artifacts/step3-smoke
          cp /tmp/step3_smoke_*.json .artifacts/step3-smoke/ 2>/dev/null || true

      - name: Upload failure artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: nightly-failure-logs-${{ github.run_number }}
          path: |
            .artifacts/openmemory-sync-report.json
            .artifacts/compose-config.yml
            .artifacts/compose-logs.txt
            .artifacts/compose-ps.txt
            .artifacts/step3-smoke/
            .artifacts/step3-diagnostics/
            .artifacts/unified-stack/
            .artifacts/build/
            .artifacts/step1-integration/
            .artifacts/openmemory-sync-check-report.json
            .artifacts/openmemory-sync-apply-report.json
            .artifacts/openmemory-sync-verify-report.json
          retention-days: 14

      # ========================================================================
      # Summary
      # ========================================================================
      - name: Generate Summary
        if: always()
        run: |
          echo "## Nightly Build Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Security Alertï¼ˆçªå‡ºæ˜¾ç¤ºï¼‰
          if [ "${{ steps.openmemory_drift_check.outputs.has_security_update }}" == "true" ]; then
            echo "> [!CAUTION]" >> $GITHUB_STEP_SUMMARY
            echo "> **Security Update Available**: OpenMemory ä¸Šæ¸¸å­˜åœ¨å®‰å…¨æ›´æ–°ï¼Œè¯·å°½å¿«è¯„ä¼°å‡çº§ï¼" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "| Step | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Deploy | ${{ steps.deploy.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Step1 Integration (MinIO) | ${{ steps.step1_integration.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Re-deploy | ${{ steps.redeploy.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Verify Unified | ${{ steps.verify_unified.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| OpenMemory Patch Check | ${{ steps.openmemory_patch_check.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| OpenMemory Drift Check | ${{ steps.openmemory_drift_check.outcome }} (allowed failure) |" >> $GITHUB_STEP_SUMMARY
          echo "| Gateway Integration | ${{ steps.gateway_integration.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Step3 PGVector | ${{ steps.step3_pgvector.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Step3 Migrate (dry-run) | ${{ steps.step3_migrate.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Step3 Smoke Test | ${{ steps.step3_smoke.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Step3 Nightly Rebuild | ${{ steps.step3_nightly_rebuild.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Step3 Dual-Read | ${{ steps.step3_dual_read.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Step3 Migration Drill | ${{ steps.step3_migration_drill.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Docker Build | ${{ steps.verify_build.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # OpenMemory Sync Check è¯¦æƒ…
          echo "### OpenMemory Sync Check" >> $GITHUB_STEP_SUMMARY
          SYNC_STATUS="${{ steps.patch_sync_parse.outputs.overall_status }}"
          if [ "$SYNC_STATUS" == "error" ]; then
            echo "> [!CAUTION]" >> $GITHUB_STEP_SUMMARY
            echo "> **çŠ¶æ€: ERROR** - å­˜åœ¨å¿…é¡»ä¿®å¤çš„é—®é¢˜" >> $GITHUB_STEP_SUMMARY
          elif [ "$SYNC_STATUS" == "warn" ]; then
            echo "> [!WARNING]" >> $GITHUB_STEP_SUMMARY
            echo "> **çŠ¶æ€: WARN** - å­˜åœ¨éœ€è¦å…³æ³¨çš„é—®é¢˜" >> $GITHUB_STEP_SUMMARY
          else
            echo "> [!NOTE]" >> $GITHUB_STEP_SUMMARY
            echo "> **çŠ¶æ€: OK** - æ‰€æœ‰æ£€æŸ¥é€šè¿‡" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| æŒ‡æ ‡ | å€¼ |" >> $GITHUB_STEP_SUMMARY
          echo "|------|------|" >> $GITHUB_STEP_SUMMARY
          echo "| Category A å†²çª | ${{ steps.patch_sync_parse.outputs.category_mismatch_A }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Category B å†²çª | ${{ steps.patch_sync_parse.outputs.category_mismatch_B }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Category C å†²çª | ${{ steps.patch_sync_parse.outputs.category_mismatch_C }} |" >> $GITHUB_STEP_SUMMARY
          echo "| å†²çªæ–‡ä»¶æ•° | ${{ steps.patch_sync_parse.outputs.conflict_files_count }} |" >> $GITHUB_STEP_SUMMARY
          echo "| å†²çªäº§ç‰©ç›®å½• | \`${{ steps.patch_sync_parse.outputs.conflict_artifacts_dir }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| strict_patch_files | ${{ steps.patch_sync_parse.outputs.strict_patch_files }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Drift Check è¯¦æƒ…
          if [ -f ".artifacts/openmemory-upstream-drift.json" ]; then
            echo "### OpenMemory Upstream Drift" >> $GITHUB_STEP_SUMMARY
            PRIORITY="${{ steps.openmemory_drift_check.outputs.priority }}"
            IS_FROZEN="${{ steps.openmemory_drift_check.outputs.is_frozen }}"
            echo "- Priority: **${PRIORITY:-unknown}**" >> $GITHUB_STEP_SUMMARY
            echo "- Frozen: **${IS_FROZEN:-unknown}**" >> $GITHUB_STEP_SUMMARY
            if [ "${{ steps.openmemory_drift_check.outputs.has_security_update }}" == "true" ]; then
              echo "- ðŸš¨ **Action Required**: Security update detected (High Priority Issue created)" >> $GITHUB_STEP_SUMMARY
            fi
            if [ "${{ steps.openmemory_drift_check.outputs.needs_override }}" == "true" ]; then
              echo "- âš ï¸ **Freeze Override Required**: å‡çº§å·²å†»ç»“ï¼Œéœ€è¦äººå·¥ override" >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          # è¾“å…¥å‚æ•°ï¼ˆä»… workflow_dispatchï¼‰
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "### Input Parameters" >> $GITHUB_STEP_SUMMARY
            echo "- run_build: ${{ github.event.inputs.run_build }}" >> $GITHUB_STEP_SUMMARY
            echo "- run_degradation: ${{ github.event.inputs.run_degradation }}" >> $GITHUB_STEP_SUMMARY
            echo "- run_minio_tests: ${{ github.event.inputs.run_minio_tests }}" >> $GITHUB_STEP_SUMMARY
            echo "- run_step3_pgvector_tests: ${{ github.event.inputs.run_step3_pgvector_tests }}" >> $GITHUB_STEP_SUMMARY
            echo "- run_step3_migration_rehearsal: ${{ github.event.inputs.run_step3_migration_rehearsal }}" >> $GITHUB_STEP_SUMMARY
            echo "- run_step3_smoke_with_check: ${{ github.event.inputs.run_step3_smoke_with_check }}" >> $GITHUB_STEP_SUMMARY
            echo "- run_nightly_rebuild: ${{ github.event.inputs.run_nightly_rebuild }}" >> $GITHUB_STEP_SUMMARY
            echo "- run_dual_read_test: ${{ github.event.inputs.run_dual_read_test }}" >> $GITHUB_STEP_SUMMARY
            echo "- run_step3_migration_drill: ${{ github.event.inputs.run_step3_migration_drill }}" >> $GITHUB_STEP_SUMMARY
          fi
